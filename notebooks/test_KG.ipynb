{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "503b7113",
   "metadata": {},
   "source": [
    "## Notebook to test and experiment with KnowledgeGroundedDecoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b999fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, PretrainedConfig, GenerationConfig\n",
    "import utils.logging as logging\n",
    "logging.set_log_level(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.knowledge_grounded_generator.kg_model import KnowledgeGroundedDecoder, KG_loss\n",
    "from models.knowledge_grounded_generator.kg_utils import ConceptGraph\n",
    "from dataset.msc_kg_sessions import KG_enriched_MSC_Session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c082aefe",
   "metadata": {},
   "source": [
    "### Define a mini test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b396184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small handmade (mini-)dataset, to facilitate testing\n",
    "\n",
    "class Mini_dataset:\n",
    "\n",
    "    def __init__(self, speaker_prefixes=['', '']):\n",
    "        self.speaker_prefixes = speaker_prefixes\n",
    "        self.data = [\n",
    "            {\n",
    "                \"text\": \"Hi, how are you doing?\", \n",
    "                \"labels\": [\"I'm good, how are you?\"],\n",
    "            }, {\n",
    "                \"text\": \"Shall we play soccer?\", \n",
    "                \"labels\": [\"It is fun and a great sport to play as a team\"],\n",
    "            }, {\n",
    "                \"text\": \"The dinner was great, but now I want to go home.\", \n",
    "                \"labels\": [\"Yes, the food was delicious\"],\n",
    "            }\n",
    "        ]\n",
    "        # print(\"0: <{}>, 1: <{}>\".format(self.speaker_prefixes[0], self.speaker_prefixes[1]))\n",
    "    def __getitem__(self, i):\n",
    "        x = self.speaker_prefixes[1] + self.data[i]['text']\n",
    "        y = self.speaker_prefixes[0] + self.data[i]['labels'][0]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "149bc05e",
   "metadata": {},
   "source": [
    "### Test impact of left versus right padding with just GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f705080",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tok_padleft = AutoTokenizer.from_pretrained(\"gpt2\", padding_side='left')\n",
    "tok_padleft.pad_token = tok_padleft.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config=GenerationConfig(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    output_hidden_states=True,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=20\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d1efed4",
   "metadata": {},
   "source": [
    "Test with very simple input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=torch.tensor(tokenizer(\" I you we\").input_ids, dtype=torch.long).view(3,1)\n",
    "attn=torch.ones((3,1), dtype=torch.long)\n",
    "pos=torch.zeros((3,1), dtype=torch.long)\n",
    "# ids, attn, pos\n",
    "logits=gpt2(input_ids=ids, attention_mask=attn, position_ids=pos).logits\n",
    "gen_out = gpt2.generate(input_ids=ids, attention_mask=attn, generation_config=generation_config)\n",
    "logits[:, 0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d311c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=torch.tensor([[50256, 314], [50256, 345], [50256, 356]], dtype=torch.long).view(3,2)\n",
    "attn=torch.tensor([[0, 1], [0, 1], [0, 1]], dtype=torch.long).view(3,2)\n",
    "pos=torch.zeros((3,2), dtype=torch.long)\n",
    "# ids, attn, pos\n",
    "logits2=gpt2(input_ids=ids, attention_mask=attn, position_ids=pos).logits\n",
    "gen_out2 = gpt2.generate(input_ids=ids, attention_mask=attn, generation_config=generation_config)\n",
    "logits2[:, 1, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0872f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "((logits2[:, 1, :]-logits[:, 0, :]).abs() > 0.001).sum().item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4e12e93",
   "metadata": {},
   "source": [
    "Since output of logits and logits2 is (almost) identical, the appears that using left_padding combined with attention works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467795a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=torch.tensor([[314, 50256], [345, 50256], [356, 50256]], dtype=torch.long).view(3,2)\n",
    "attn=torch.tensor([[1, 0], [1, 0], [1, 0]], dtype=torch.long).view(3,2)\n",
    "pos=torch.zeros((3,2), dtype=torch.long)\n",
    "# ids, attn, pos\n",
    "logits3=gpt2(input_ids=ids, attention_mask=attn, position_ids=pos).logits\n",
    "gen_out3 = gpt2.generate(input_ids=ids, attention_mask=attn, generation_config=generation_config)\n",
    "logits3[:, 0, :5], logits3[:, 1, :5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97102c0",
   "metadata": {},
   "source": [
    "Two two tensors are different. So even though attention value for those tokens is zero, and the token itself is padding token, the forward function still generates a different output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e01cc84",
   "metadata": {},
   "source": [
    "Conclusion: left padding and right padding gives different results from the forward function. Even when position ids and attention mask is adjusted to 'correct' for the differences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc879355",
   "metadata": {},
   "source": [
    "Test with short sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0996609",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The weather is\"\n",
    "enc = tokenizer(sentence, return_tensors='pt')\n",
    "enc_pad_left = enc.copy()\n",
    "enc_pad_left = {\n",
    "    'input_ids': torch.cat([enc.input_ids, torch.tensor([tokenizer.pad_token_id] * 5).view(1,5)], dim=1),\n",
    "    'attention_mask': torch.cat([enc.attention_mask, torch.zeros((1,5), dtype=torch.long)], dim=1)\n",
    "}\n",
    "enc_pad_right = {\n",
    "    'input_ids': torch.cat([torch.tensor([tokenizer.pad_token_id] * 5).view(1,5), enc.input_ids], dim=1),\n",
    "    'attention_mask': torch.cat([torch.zeros((1,5), dtype=torch.long), enc.attention_mask], dim=1)\n",
    "}\n",
    "enc, enc_pad_left, enc_pad_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out = gpt2.generate(**enc, generation_config=generation_config)\n",
    "gen_padleft = gpt2.generate(**enc_pad_left, generation_config=generation_config)\n",
    "gen_padright = gpt2.generate(**enc_pad_right, generation_config=generation_config)\n",
    "gen_out, gen_padleft, gen_padright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4590460",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = tokenizer.batch_decode(gen_out)\n",
    "resp_padleft = tokenizer.batch_decode(gen_padleft)\n",
    "resp_padright = tokenizer.batch_decode(gen_padright)\n",
    "resp, resp_padleft, resp_padright"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a594159",
   "metadata": {},
   "source": [
    "Test with mini dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = Mini_dataset()\n",
    "text_batch = [testdata[i] for i in range(len(testdata))]\n",
    "text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*text_batch)\n",
    "encoded_x = tokenizer(text=x, padding=True, return_tensors='pt')\n",
    "lens = torch.cumsum(encoded_x.attention_mask, dim=1)[:, -1]\n",
    "position_ids = (torch.cumsum(encoded_x.attention_mask, dim=1) - 1).clip(0)\n",
    "position_ids = position_ids[:, -encoded_x.input_ids.shape[1]:]\n",
    "encoded_x.input_ids.shape, lens, encoded_x, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4747da",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_x_padleft = tok_padleft(text=[item + tokenizer.eos_token for item in x], padding=True, return_tensors='pt')\n",
    "# encoded_x_padleft.input_ids = torch.cat([encoded_x_padleft.input_ids, torch.full((3,1), tokenizer.eos_token_id)], dim=1)\n",
    "# encoded_x_padleft.attention_mask = torch.cat([encoded_x_padleft.attention_mask, torch.ones((3,1))], dim=1)\n",
    "\n",
    "position_ids_padleft = (torch.cumsum(encoded_x_padleft.attention_mask, dim=1) - 1).clip(0)\n",
    "position_ids_padleft = position_ids_padleft[:, -encoded_x_padleft.input_ids.shape[1]:]\n",
    "encoded_x_padleft.input_ids.shape, encoded_x_padleft, position_ids_padleft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76833bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd = gpt2(**encoded_x, position_ids=position_ids)\n",
    "fwd.logits[:, -1, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_left = gpt2(**encoded_x_padleft, position_ids=position_ids_padleft)\n",
    "fwd_left.logits[:, -1, :10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b49c204",
   "metadata": {},
   "source": [
    "The logits for the last token in the batch is diffent, despite passing attention_mask and position_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54994c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd.logits[1, 5, :10], fwd_left.logits[1, -2, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e12706",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd.logits[2, 12, :10], fwd_left.logits[2, -2, :10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15d15655",
   "metadata": {},
   "source": [
    "Comparing the 6th token of second sentence (right padded), with the last token of second sentence, before the oes_token (right padded) --> equal !!\n",
    "Same for 12th token of third sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bf04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out = gpt2.generate(**encoded_x, generation_config=generation_config)\n",
    "gen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out_padleft = gpt2.generate(**encoded_x_padleft, generation_config=generation_config)\n",
    "gen_out_padleft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(gen_out)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_padleft = tokenizer.batch_decode(gen_out_padleft)\n",
    "response_padleft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8086be7",
   "metadata": {},
   "source": [
    "### Test with DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bec47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogpt_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\", padding_side='left')\n",
    "dialogpt = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "dialogpt_tokenizer.pad_token = tokenizer.eos_token\n",
    "dialogpt_tokenizer.bos_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogpt.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = Mini_dataset()\n",
    "text_batch = [testdata[i] for i in range(len(testdata))]\n",
    "text_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config=GenerationConfig(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    output_hidden_states=True,\n",
    "    use_cache=True,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e28cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*text_batch)\n",
    "encoded_x_dgpt = dialogpt_tokenizer(text=[item + tokenizer.eos_token for item in x], padding=True, return_tensors='pt')\n",
    "encoded_x_dgpt.input_ids.shape, encoded_x_dgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_x_dgpt.input_ids - encoded_x_padleft.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8aee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out_dgpt = dialogpt.generate(**encoded_x_dgpt, generation_config=generation_config)\n",
    "gen_out_dgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6998ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dgpt = tokenizer.batch_decode(gen_out_dgpt)\n",
    "response_dgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out_list = [dialogpt.generate(**dialogpt_tokenizer(item + tokenizer.eos_token, return_tensors='pt'), generation_config=generation_config) for item in x]\n",
    "gen_out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_list = [tokenizer.batch_decode(g) for g in gen_out_list]\n",
    "resp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bff3adb",
   "metadata": {},
   "source": [
    "### Now use same mini dataset, but with speaker prefixes added before the utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = Mini_dataset(['<me>', '<you>'])\n",
    "text_batch = [testdata[i] for i in range(len(testdata))]\n",
    "text_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28d4b2bc",
   "metadata": {},
   "source": [
    "### Now use same mini dataset, but with extra tokens added to tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_prefixes = ['<me>', '<you>']\n",
    "tok_padleft.add_tokens(speaker_prefixes)\n",
    "dialogpt_tokenizer.add_tokens(speaker_prefixes)\n",
    "gpt2.resize_token_embeddings(len(tok_padleft))\n",
    "dialogpt.resize_token_embeddings(len(tokenizer))\n",
    "tok_padleft.convert_tokens_to_ids(speaker_prefixes), dialogpt_tokenizer.convert_tokens_to_ids(speaker_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f86560",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*text_batch)\n",
    "encoded_x = tokenizer(text=x, padding=True, return_tensors='pt')\n",
    "encoded_x.input_ids.shape, encoded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_out2 = gpt2.generate(\n",
    "    **encoded_x,\n",
    "    generation_config=GenerationConfig(\n",
    "        pad_token_id=gpt2.config.eos_token_id,\n",
    "        output_hidden_states=True,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=20\n",
    "    ))\n",
    "gen_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(gen_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2083f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ('I', 'You', 'We')\n",
    "encoded_x = tokenizer(text=x, padding=True, return_tensors='pt')\n",
    "encoded_x.input_ids.shape, encoded_x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0de82c46",
   "metadata": {},
   "source": [
    "### Set up KnowledgeGroundedDecoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "# NOTE: Tokenizer uses LEFT padding\n",
    "\n",
    "# lm = \"microsoft/DialoGPT-small\"\n",
    "lm = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(lm, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.bos_token = tokenizer.eos_token\n",
    "speaker_prefixes = None # ['<self>', '<other>']\n",
    "add_tokens = None # speaker_prefixes\n",
    "if add_tokens is not None:\n",
    "    num_added_toks = tokenizer.add_tokens(add_tokens)\n",
    "\n",
    "opt = {\n",
    "    \"lm\": lm,\n",
    "    \"bos_token_id\": tokenizer.bos_token,\n",
    "    \"num_hops\": 2,\n",
    "    \"aggregate_method\": \"max\",\n",
    "    \"alpha\": 0.7,\n",
    "    \"beta\": 0.2,\n",
    "    \"gamma\": 0.33,\n",
    "    'fixed_lm': False,\n",
    "    'block_src': True,\n",
    "    'gate': 0.0 # Gate=0.0 means output should be equal to regular GPT2 output\n",
    "}\n",
    "\n",
    "model = KnowledgeGroundedDecoder(opt, tokenizer, config=PretrainedConfig())\n",
    "model.gpt2model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a51f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gpt2model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the dataset\n",
    "\n",
    "kg_datadir = '/users/FrankVerhoef/Programming/PEX/data/kg_data/'\n",
    "opt_dataset = {\n",
    "    'kg_datadir': kg_datadir, \n",
    "    'dataset_concepts': 'dataset_concepts.txt', \n",
    "    'kg': 'kg.graph', \n",
    "    \"speaker_prefixes\": speaker_prefixes,\n",
    "    \"include_persona\": False,\n",
    "    \"max_concepts\": 256,\n",
    "    \"max_triples\": 768,\n",
    "    \"max_branch\": 64,\n",
    "    \"overlapping_concepts\": \"excl-src-in-tgt\",\n",
    "    \"num_hops\": 2,\n",
    "}\n",
    "\n",
    "kg = ConceptGraph(path=kg_datadir, graph='kg.graph')\n",
    "kg.build_reduced_graph(kg_datadir + 'dataset_concepts.txt')\n",
    "\n",
    "basedir = '/Users/FrankVerhoef/Programming/PEX/data/msc/msc_dialogue/'\n",
    "dataset = KG_enriched_MSC_Session(\n",
    "    opt_dataset, \n",
    "    basedir=basedir, \n",
    "    sessions=['1-both-revised-no_cands'],\n",
    "    subset='valid',\n",
    "    tokenizer=tokenizer, \n",
    "    kg=kg,\n",
    "    max_samples=None, \n",
    "    batch_format=\"huggingface\", \n",
    "    batch_pad_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c7a2cb2",
   "metadata": {},
   "source": [
    "### First test with small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small handmade (mini-)dataset, to facilitate testing\n",
    "\n",
    "class Mini_dataset:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = [\n",
    "            {\n",
    "                \"text\": \"I like my mother and sister. It is good to be with them.\", \n",
    "                \"labels\": [\"Your family is important since birth\"],\n",
    "            }, {\n",
    "                \"text\": \"Shall we play soccer?\", \n",
    "                \"labels\": [\"It is fun and a great sport to play as a team\"],\n",
    "            }, {\n",
    "                \"text\": \"The dinner was great, but now I want to go home.\", \n",
    "                \"labels\": [\"Yes, the food was delicious\"],\n",
    "            }\n",
    "        ]\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]['text'], self.data[i]['labels']\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "testdata = Mini_dataset()\n",
    "\n",
    "# Enrich the minidataset with information about related concepts, from the knowledge graph\n",
    "enriched = [(*testdata[i], dataset._get_kg_info(*testdata[i])) for i in range(len(testdata))]\n",
    "enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da891cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the concept_token_ids that are marked with label==1 occur in the target sentence\n",
    "\n",
    "tokenizer.decode([\n",
    "    c_id \n",
    "    for c_id, label in zip(enriched[0][2]['concept_token_ids'], enriched[0][2]['concept_labels'])\n",
    "    if label == 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1418015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output of the tokenizer\n",
    "# NOTE: tensors are LEFT-padded\n",
    "\n",
    "tokenizer(text=[testdata[i][0] for i in range(len(testdata))], padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset items to a batch\n",
    "\n",
    "batch = dataset.batchify(enriched)\n",
    "inputs, labels, kg_input = batch\n",
    "L = inputs.input_ids.shape[1]\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "# Check if the output equals result in previous cell\n",
    "print(input_ids.shape)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the input_ids and the KG-input to generate text\n",
    "output = model.generate(\n",
    "    inputs=torch.cat([input_ids, torch.full((3,1), tokenizer.bos_token_id)], dim=1),\n",
    "    kg_input=kg_input,\n",
    "    generation_config=GenerationConfig(\n",
    "        pad_token_id=model.gpt2model.config.eos_token_id,\n",
    "        output_hidden_states=True,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check output size: dimension 1 must be at most 10 tokens larger (=max_new_tokens)\n",
    "print(output.shape)\n",
    "\n",
    "# Output the newly generated tokens are concatenated after the original input_ids\n",
    "for context, out in zip(enriched, output):\n",
    "    print(\"Context:  \", context[0])\n",
    "#     print(\"Label:    \", context[1])\n",
    "    print(\"Tensor:   \", out)\n",
    "    print(\"Response: \", dataset.tokenizer.batch_decode(out))\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2762304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is exactly the same 'generate', but now using the generate function of gpt2model directly\n",
    "output = model.gpt2model.generate(\n",
    "    inputs=input_ids,\n",
    "    generation_config=GenerationConfig(\n",
    "        pad_token_id=model.gpt2model.config.eos_token_id,\n",
    "        output_hidden_states=True,\n",
    "        output_scores=True,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=10,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    ")\n",
    "print(output.keys())\n",
    "for context, out in zip(enriched, output.sequences):\n",
    "    print(\"Context:  \", context[0])\n",
    "#     print(\"Label:    \", context[1])\n",
    "    print(\"Tensor:   \", out)\n",
    "    print(\"Response: \", dataset.tokenizer.batch_decode(out))\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores is a tuple with length 10 (because 10 generated tokens). \n",
    "# Each element a batch of the scores\n",
    "scores = torch.cat(output.scores).reshape((3, 10, -1))\n",
    "print(scores.shape)\n",
    "top_5_indices = torch.topk(scores, k=5, dim=2, sorted=True).indices\n",
    "print(top_5_indices)\n",
    "for sequence in top_5_indices:\n",
    "    for top5 in sequence:\n",
    "        print(' '.join([\"{:10s}\".format(token) for token in tokenizer.convert_ids_to_tokens(top5)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2caee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use forward to generate logits to determine the next token\n",
    "\n",
    "output = model.forward(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    kg_input=kg_input\n",
    ")\n",
    "print(output.logits.shape)\n",
    "print(inputs.input_ids)\n",
    "print(inputs.attention_mask)\n",
    "print(output.last_hidden_state.shape)\n",
    "print(output.logits.argmax(dim=-1))\n",
    "\n",
    "# The next token of a sequence is determined by the last hidden state of the last token of each sequence\n",
    "print(tokenizer.batch_decode(output.logits[:, -1, :].argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same check, but using the forward function of the gpt2model directly\n",
    "\n",
    "attention_mask = inputs.attention_mask\n",
    "position_ids = (torch.cumsum(attention_mask, dim=1) - 1).clip(0)\n",
    "position_ids = position_ids[:, -input_ids.shape[1]:]\n",
    "output = model.gpt2model.forward(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids\n",
    ")\n",
    "print(output.logits.argmax(dim=-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f04a22",
   "metadata": {},
   "source": [
    "### Now test with trained model and MSC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = KG_loss(ignore_index=tokenizer.pad_token_id, invalid=-1, alpha=opt['alpha'], beta=opt['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=3, shuffle=False, collate_fn=dataset.batchify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(valid_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fecb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- input ---\")\n",
    "inp = tokenizer.batch_decode(batch[0].input_ids)\n",
    "for i in inp:\n",
    "    print(i)\n",
    "print(\"--- labels ---\")\n",
    "lbl = tokenizer.batch_decode(batch[1].input_ids)\n",
    "for i in lbl:\n",
    "    print(i)\n",
    "# batch[0].input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.valid_step(batch, criterion=criterion, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/users/FrankVerhoef/Programming/PEX/checkpoints/'\n",
    "load = 'test_kgg'\n",
    "logging.info(\"Loading model from {}\".format(checkpoint_dir + load))\n",
    "model.load_state_dict(torch.load(checkpoint_dir + load, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e39678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.valid_step(batch, criterion=criterion, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs, labels, kg_input = batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.forward(\n",
    "        input_ids=torch.cat([inputs.input_ids, labels.input_ids], dim=1),\n",
    "        attention_mask=torch.cat([inputs.attention_mask, labels.attention_mask], dim=1),\n",
    "        kg_input=kg_input\n",
    "    )\n",
    "    len_labels = labels.input_ids.shape[1]\n",
    "    loss, gen_loss, triple_loss, gate_loss = criterion(\n",
    "        output.logits[:, -len_labels:], labels.input_ids, \n",
    "        output.triple_prob[:, -len_labels:], kg_input.triple_labels, \n",
    "        output.gate[:, -len_labels:], kg_input.gate_labels\n",
    "    )\n",
    "\n",
    "pred = output.logits[:, -len_labels:].argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- responses ---\")\n",
    "resp = tokenizer.batch_decode(pred)\n",
    "for i in resp:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM accuracy\n",
    "token_correct = labels['input_ids'].eq(pred) * labels['attention_mask']\n",
    "token_acc = (token_correct.sum() / labels['attention_mask'].sum()).item() \n",
    "token_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30131845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the input_ids and the KG-input to generate text\n",
    "gen_output = model.generate(\n",
    "    inputs=inputs.input_ids,\n",
    "    kg_input=kg_input,\n",
    "    generation_config=GenerationConfig(\n",
    "        pad_token_id=model.gpt2model.config.eos_token_id,\n",
    "        output_hidden_states=True,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        # decoder_start_token_id=tokenizer.convert_tokens_to_ids('<self>'),\n",
    "        max_new_tokens=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check output size: dimension 1 must be at most 10 tokens larger (=max_new_tokens)\n",
    "print(gen_output.shape)\n",
    "\n",
    "# Output the newly generated tokens are concatenated after the original input_ids\n",
    "for inp, lbl, out in zip(inputs.input_ids, labels.input_ids, gen_output):\n",
    "    print(\"Context:  \", inp)\n",
    "    print(\"Label:    \", lbl)\n",
    "    print(\"Tensor:   \", out)\n",
    "    print(\"Response: \", dataset.tokenizer.batch_decode(out))\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([inputs.input_ids, labels.input_ids[:, 0].view(-1, 1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- input ---\")\n",
    "inp = tokenizer.batch_decode(inputs.input_ids)\n",
    "for i in inp:\n",
    "    print(i)\n",
    "print(\"--- labels ---\")\n",
    "lbl = tokenizer.batch_decode(labels.input_ids)\n",
    "for i in lbl:\n",
    "    print(i)\n",
    "# batch[0].input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f28b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.valid_step(batch, criterion=criterion, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335930c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- responses ---\")\n",
    "resp = tokenizer.batch_decode(pred)\n",
    "for i in resp:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_output[:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78836a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids('<self>')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46b03a4e",
   "metadata": {},
   "source": [
    "### Test DialoGPT (example from Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6230ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d56de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    print(bot_input_ids)\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17495437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
