{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from models.bart_extractor import BartExtractor, ExtractedFactLoss\n",
    "from dataset.msc_summary_turns import MSC_Turns\n",
    "from dataset.msc_summary import MSC_Summaries\n",
    "from metrics.terp import TerpMetric\n",
    "from metrics.nli import NLIMetric\n",
    "\n",
    "import utils.logging as logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-02 20:28:48,035 INFO     | Loading model from /Users/FrankVerhoef/Programming/PEX/checkpoints/trained_nll05_bart\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.set_log_level(logging.SPAM)\n",
    "\n",
    "# Settings for dataset\n",
    "datadir = '/Users/FrankVerhoef/Programming/PEX/data/'\n",
    "basedir = 'msc/msc_personasummary/'\n",
    "bart_base = 'facebook/bart-base'\n",
    "sessions = [1]\n",
    "len_context = 2\n",
    "speaker_prefixes = [\"<other>\", \"<self>\"]\n",
    "nofact_token = '' #'<nofact>'\n",
    "add_tokens = None #speaker_prefixes + [nofact_token]\n",
    "test_samples = 20\n",
    "subset = 'train'\n",
    "\n",
    "# config for TerpMetric\n",
    "JAVA_HOME = \"/opt/homebrew/opt/openjdk/libexec/openjdk.jdk/Contents/Home\"\n",
    "TERPDIR = \"/Users/FrankVerhoef/Programming/terp/\"\n",
    "TMPDIR = \"/Users/FrankVerhoef/Programming/PEX/output/\"\n",
    "TerpMetric.set(terp_dir=TERPDIR, java_home=JAVA_HOME, tmp_dir=TMPDIR)\n",
    "\n",
    "# Settings for model\n",
    "checkpoint_dir = '/Users/FrankVerhoef/Programming/PEX/checkpoints/'\n",
    "load = 'trained_nll05_bart'\n",
    "\n",
    "# Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(bart_base)\n",
    "if add_tokens is not None:\n",
    "    num_added_toks = tokenizer.add_tokens(add_tokens)\n",
    "nofact_token_id = tokenizer.convert_tokens_to_ids(nofact_token) if nofact_token != '' else tokenizer.eos_token_id\n",
    "assert nofact_token_id != tokenizer.unk_token_id, \"nofact_token '{}' must be known token\".format(nofact_token)\n",
    "\n",
    "model = BartExtractor(bart_base=bart_base, nofact_token_id=nofact_token_id)\n",
    "model.bart.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "MSC_Turns.set(tokenizer=tokenizer, len_context=len_context, speaker_prefixes=speaker_prefixes, nofact_token=nofact_token)\n",
    "msc_turns = MSC_Turns(basedir=datadir + basedir, sessions=sessions, subset=subset, max_samples=10)\n",
    "\n",
    "logging.info(\"Loading model from {}\".format(checkpoint_dir + load))\n",
    "model.load_state_dict(torch.load(checkpoint_dir + load, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-base\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.27.4\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bart.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"_from_model_config\": true,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"early_stopping\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"num_beams\": 4,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"transformers_version\": \"4.27.4\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bart.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"<self>Really? Do you do anything to assist you with coming out?\\n<other>I've tried bringing it up before, but something always gets in the way.\", '')\n",
      "('<self>Flying is really fun, but clouds are amazing to look at\\n<other>I agree. I like having a picnic and looking at the cloud shapes.', 'I enjoy picnics.')\n",
      "(\"<self>Oh, what is that? I'm not sure I know what you are talking about?\\n<other>Ll, I'm off of alot of coffee. Sorry. I can tell this will be interesting\", 'I drink a lot of coffee.')\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(msc_turns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-02 20:28:52,410 INFO     | Evaluating model on 10 samples of testdata in msc/msc_personasummary/ with arguments {'generation_config': {'max_new_tokens': 20}, 'device': 'cpu', 'log_interval': 10}\n",
      "2023-09-02 20:28:53,295 SPAM     | Generate: pred_fact=tensor([False])\n",
      "2023-09-02 20:28:53,296 SPAM     | Generate: gen_out=tensor([[2, 0, 2]])\n",
      "context:     <self>Really? Do you do anything to assist you with coming out?\n",
      "<other>I've tried bringing it up before, but something always gets in the way.\n",
      "target:      \n",
      "prediction:  \n",
      "----------------------------------------\n",
      "2023-09-02 20:28:54,023 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:54,024 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,   101,   519,    10, 22297,     4,     2]])\n",
      "context:     <self>Flying is really fun, but clouds are amazing to look at\n",
      "<other>I agree. I like having a picnic and looking at the cloud shapes.\n",
      "target:      I enjoy picnics.\n",
      "prediction:  I like having a picnic.\n",
      "----------------------------------------\n",
      "2023-09-02 20:28:54,764 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:54,764 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,  4076, 47375,     9,  3895,     4,     2]])\n",
      "context:     <self>Oh, what is that? I'm not sure I know what you are talking about?\n",
      "<other>Ll, I'm off of alot of coffee. Sorry. I can tell this will be interesting\n",
      "target:      I drink a lot of coffee.\n",
      "prediction:  I drink alot of coffee.\n",
      "----------------------------------------\n",
      "2023-09-02 20:28:55,523 SPAM     | Generate: pred_fact=tensor([False])\n",
      "2023-09-02 20:28:55,524 SPAM     | Generate: gen_out=tensor([[2, 0, 2]])\n",
      "context:     <self>Yes, I lived in south florida for years.\n",
      "<other>Very cool! Nice chatting with you!\n",
      "target:      \n",
      "prediction:  \n",
      "----------------------------------------\n",
      "2023-09-02 20:28:56,243 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:56,244 SPAM     | Generate: gen_out=tensor([[   2,    0, 2387, 1150,   16,   10, 3331,    4,    2]])\n",
      "context:     <self>I also do photography, it helps relieve the stress.\n",
      "<other>It does sound relaxing I caught the writing bug from my father\n",
      "target:      I learned to write from my father.\n",
      "prediction:  My father is a writer.\n",
      "----------------------------------------\n",
      "2023-09-02 20:28:57,072 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:57,072 SPAM     | Generate: gen_out=tensor([[   2,    0,  100,  101,  592, 2787,   19,   92,   82,    4,    2]])\n",
      "context:     <self>Is there anything else you like to do?\n",
      "<other>I love socializing with new people. Also, finding cool gadgets\n",
      "target:      I love socializing with new people. I like finding cool gadgets.\n",
      "prediction:  I like socializing with new people.\n",
      "----------------------------------------\n",
      "2023-09-02 20:28:57,741 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:57,742 SPAM     | Generate: gen_out=tensor([[   2,    0, 2387,  512,   16, 6907,    4,    2]])\n",
      "context:     <self>I do not own a car, only my moped\n",
      "<other>I could recover the seat color though, in pink\n",
      "target:      \n",
      "prediction:  My car is pink.\n",
      "----------------------------------------\n",
      "2023-09-02 20:28:58,658 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:58,659 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,    33,  3033,   583,   127,    11, 18712,  2330,\n",
      "             4,     2]])\n",
      "context:     <self>Me too living near her parents makes it iffy\n",
      "<other>I've lived near my inlaws twice. It can suck or it can be awesome\n",
      "target:      I've lived near my inlaws twice.\n",
      "prediction:  I have lived near my inlaws twice.\n",
      "----------------------------------------\n",
      "2023-09-02 20:28:59,416 SPAM     | Generate: pred_fact=tensor([True])\n",
      "2023-09-02 20:28:59,416 SPAM     | Generate: gen_out=tensor([[   2,    0,  100, 8505,   94,   76,    4,    2]])\n",
      "context:     <self>To be honest, I've never held a job before but I enjoy video games.\n",
      "<other>Did you go to college for that? I graduated last year\n",
      "target:      I graduated from college.\n",
      "prediction:  I graduated last year.\n",
      "----------------------------------------\n",
      "2023-09-02 20:29:00,000 SPAM     | Generate: pred_fact=tensor([False])\n",
      "2023-09-02 20:29:00,000 SPAM     | Generate: gen_out=tensor([[2, 0, 2]])\n",
      "context:     <self>Hi wats up its miguel\n",
      "<other>Hi miguel, how are you?\n",
      "target:      \n",
      "prediction:  \n",
      "----------------------------------------\n",
      "2023-09-02 20:29:00,001 VERBOSE  | Evaluated 10/10 samples\n",
      "2023-09-02 20:29:00,001 INFO     | Start calculating stats\n",
      "2023-09-02 20:29:05,763 SPAM     | TERp output\n",
      "Loading parameters from /Users/FrankVerhoef/Programming/terp/data/terpa.param\n",
      "Loading parameters from /Users/FrankVerhoef/Programming/terp/data/data_loc.param\n",
      "\"/Users/FrankVerhoef/Programming/PEX/output/hyp.trans\" was successfully parsed as Trans text\n",
      "\"/Users/FrankVerhoef/Programming/PEX/output/ref.trans\" was successfully parsed as Trans text\n",
      "Creating Segment Phrase Tables From DB\n",
      "Processing [sys][000000][000000]\n",
      "Processing [sys][000001][000000]\n",
      "Processing [sys][000002][000000]\n",
      "Processing [sys][000003][000000]\n",
      "Processing [sys][000004][000000]\n",
      "Processing [sys][000005][000000]\n",
      "Finished Calculating TERp\n",
      "Total TER: 0,58 (25,44 / 44,00)\n",
      "\n",
      "2023-09-02 20:29:06,411 SPAM     | TERp output\n",
      "Loading parameters from /Users/FrankVerhoef/Programming/terp/data/terpa.param\n",
      "Loading parameters from /Users/FrankVerhoef/Programming/terp/data/data_loc.param\n",
      "\"/Users/FrankVerhoef/Programming/PEX/output/hyp.trans\" was successfully parsed as Trans text\n",
      "\"/Users/FrankVerhoef/Programming/PEX/output/ref.trans\" was successfully parsed as Trans text\n",
      "Creating Segment Phrase Tables From DB\n",
      "Processing [sys][000000][000000]\n",
      "Processing [sys][000001][000000]\n",
      "Processing [sys][000002][000000]\n",
      "Processing [sys][000003][000000]\n",
      "Processing [sys][000003][000001]\n",
      "Processing [sys][000004][000000]\n",
      "Processing [sys][000005][000000]\n",
      "Finished Calculating TERp\n",
      "Total TER: 0,60 (22,07 / 37,00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config for TerpMetric\n",
    "JAVA_HOME = \"/opt/homebrew/opt/openjdk/libexec/openjdk.jdk/Contents/Home\"\n",
    "TERPDIR = \"/Users/FrankVerhoef/Programming/terp/\"\n",
    "TMPDIR = \"/Users/FrankVerhoef/Programming/PEX/output/\"\n",
    "TerpMetric.set(terp_dir=TERPDIR, java_home=JAVA_HOME, tmp_dir=TMPDIR)\n",
    "\n",
    "# set config for NLIMetric\n",
    "NLIMetric.set(nli_model='facebook/bart-large-mnli', device='cpu', batch_size=8)\n",
    "\n",
    "eval_kwargs = {\n",
    "    'generation_config': {\n",
    "        \"max_new_tokens\": 20,\n",
    "    },\n",
    "    'device': 'cpu', \n",
    "    'log_interval': 10\n",
    "}\n",
    "\n",
    "logging.info(\"Evaluating model on {} samples of testdata in {} with arguments {}\".format(len(msc_turns), basedir, eval_kwargs))\n",
    "eval_stats, result_dict = msc_turns.evaluate(model, **eval_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8999999761581421,\n",
       " 'f1': 0.9230769276618958,\n",
       " 'precision': 0.8571428656578064,\n",
       " 'recall': 1.0,\n",
       " 'cm': [[3, 1], [0, 6]],\n",
       " 'nli_predictions': 0.7534764965031562,\n",
       " 'nli_targets': 0.9005989134311676,\n",
       " 'nli_preds_to_targets': 0.5029634467937285,\n",
       " 'nli_targets_to_preds': 0.8067612039546171,\n",
       " 'numwords_factor': 0.9877344877344877,\n",
       " 'bleu_2': 0.4276726543903351,\n",
       " 'bleu_4': 0.31776177883148193,\n",
       " 'terp': 0.5354940493901571,\n",
       " 'rouge1_fmeasure': 0.5622718930244446,\n",
       " 'rouge1_precision': 0.6261904835700989,\n",
       " 'rouge1_recall': 0.5313853025436401,\n",
       " 'rouge2_fmeasure': 0.3629629611968994,\n",
       " 'rouge2_precision': 0.42499998211860657,\n",
       " 'rouge2_recall': 0.32777777314186096,\n",
       " 'rougeL_fmeasure': 0.5426640510559082,\n",
       " 'rougeL_precision': 0.5984126925468445,\n",
       " 'rougeL_recall': 0.5162338018417358}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSC_Summaries.set(tokenizer=tokenizer, speaker_prefixes=speaker_prefixes, nofact_token=nofact_token)\n",
    "msc_summaries = MSC_Summaries(\n",
    "    basedir=datadir + basedir, \n",
    "    session=1, \n",
    "    subset=\"test\",   \n",
    "    max_samples=5 #test_samples      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<self>Hi how are you?\\n<other>Great, yourself?', \"<self>Good, do you like to paint? I do!\\n<other>I'm not much of a painter, I prefer horses. What do you paint?\", '<self>People playing music since I like to do that as well!\\n<other>My ancestor had a song written about him. He was an american civil war general.', '<self>Wow that is really interesting.\\n<other>Do you like fruit? I love to eat fruit.', \"<self>I do like fruit! It's not expensive and it helps me to live within a budget.\\n<other>Budgets are important! Especially when dealing with such large animals as horses.\", \"<self>Oh I'm sure. I love animals, like people!\\n<other>I like people, unlike my father. He rarely smiles which makes socializing difficult.\", \"<self>Sounds like my boss. I wish I didn't have one.\\n<other>Oh, what do you do? Bosses can be tough to deal with.\"], \"I like horses.\\nI'm descended from a civil war general.\\nI love fruit.\")\n",
      "(['<self>Hello, just got back from fishing, how are you?\\n<other>Fine. I love to fish. Live by the ocean?', \"<self>Yep, get kinda tired of it sometimes. I'm a pro fisher.\\n<other>I live in florida. What do you fish for?\", \"<self>Tuna for work but whatever bites. Florida is nice. You like baseball?\\n<other>Yes, I used to live in cincinnati so, I'm a big reds fan.\", \"<self>Ah, the reds. I'm a fan even though I'm in new york.\\n<other>Yeah! I'm in the process of setting up a work from home office.\", '<self>Sorry, my lasagna was finished in the oven. What kind of vehicle do you have?\\n<other>A hyundai elantra. Good gas mileage. I really like it.', '<self>I gotta have the gas guzzling trucks!\\n<other>My fiance has a ford f150 and it eats the gas!', '<self>Oh, congrats on the engagement. When\\'s the date?\\n<other>It\\'s a \"forever engagement\". We\\'ve both been married before.'], 'I love  fishing.\\nLives in Florida.\\nI also lives in Cincinnati.\\nI like  baseball and a big Reds fan.\\nI am going to start working ffrom home.\\nDrives a Hyundai Elantra and likes it.\\nI am getting  married.\\nI have previously been married.')\n",
      "(['<self>Hey, how are you?\\n<other>Good! My sister and I are twins. Do you have any siblings?', \"<self>Yes, but we aren't twins. We do both love baseball though.\\n<other>So do I. I love the twins as we live in mn..\", \"<self>Gotcha, we're red sox fans cuz we live in new england.\\n<other>What do you do? I am a dentist.\", \"<self>Nice! I'm a truck driver.\\n<other>Tough job. Been doing it long? I've been at my work for 14 years.\", \"<self>Yeah, I've been doing it for a while. Sometimes I fish for extra money too.\\n<other>Invest in gold. That's my favorite metal for investment.\", \"<self>Nice. I've got some bitcoin, but no gold.\\n<other>That's good, too. We'll see how they do...\"], \"I have a twin sister.\\nI live in Minnesota.\\nI love the Minnesota Twins.\\nI am a dentist.\\nI've been a dentist for 14 years.\\nMy favorite investment is in gold.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(msc_summaries[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-02 20:29:06,466 INFO     | Start evaluation of model BartExtractor on metrics: ter\n",
      "2023-09-02 20:29:09,727 SPAM     | Generate: pred_fact=tensor([False,  True,  True,  True, False,  True, False])\n",
      "2023-09-02 20:29:09,728 SPAM     | Generate: gen_out=tensor([[    2,     0,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,     0,   100,   101,  8087,     4,     2,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,     0,  2387, 40701,    56,    10,  2214,  1982,    59,   123,\n",
      "             4,     2],\n",
      "        [    2,     0,   100,   657,  6231,     4,     2,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,     0,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,     0,   100,   218,    75,   101,    82,     4,     2,     1,\n",
      "             1,     1],\n",
      "        [    2,     0,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "Utterances: \n",
      "\t<self>Hi how are you?\n",
      "<other>Great, yourself?\n",
      "\t<self>Good, do you like to paint? I do!\n",
      "<other>I'm not much of a painter, I prefer horses. What do you paint?\n",
      "\t<self>People playing music since I like to do that as well!\n",
      "<other>My ancestor had a song written about him. He was an american civil war general.\n",
      "\t<self>Wow that is really interesting.\n",
      "<other>Do you like fruit? I love to eat fruit.\n",
      "\t<self>I do like fruit! It's not expensive and it helps me to live within a budget.\n",
      "<other>Budgets are important! Especially when dealing with such large animals as horses.\n",
      "\t<self>Oh I'm sure. I love animals, like people!\n",
      "<other>I like people, unlike my father. He rarely smiles which makes socializing difficult.\n",
      "\t<self>Sounds like my boss. I wish I didn't have one.\n",
      "<other>Oh, what do you do? Bosses can be tough to deal with.\n",
      "Summary: \n",
      "\tI like horses.\n",
      "\tI'm descended from a civil war general.\n",
      "\tI love fruit.\n",
      "Prediction: \n",
      "\tI like horses.\n",
      "\tMy ancestor had a song written about him.\n",
      "\tI love fruit.\n",
      "\tI don't like people.\n",
      "\n",
      "2023-09-02 20:29:13,326 SPAM     | Generate: pred_fact=tensor([True, True, True, True, True, True, True])\n",
      "2023-09-02 20:29:13,327 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,   657,     7,  3539,     4,     2,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,     0,   100,   697,    11,  1261,     4,     2,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,     0,   100,   341,     7,   697,    11,  6322,     4,    38,\n",
      "           101,  3403,     4,     2],\n",
      "        [    2,     0,   100,   173,    31,   184,     4,     2,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,     0,   100,    33,    10, 13525,  1448,   927,   763,     4,\n",
      "             2,     1,     1,     1],\n",
      "        [    2,     0,   100,    33,    10, 19960,     4,     2,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,     0,   100,   524,  4009,     4,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "Utterances: \n",
      "\t<self>Hello, just got back from fishing, how are you?\n",
      "<other>Fine. I love to fish. Live by the ocean?\n",
      "\t<self>Yep, get kinda tired of it sometimes. I'm a pro fisher.\n",
      "<other>I live in florida. What do you fish for?\n",
      "\t<self>Tuna for work but whatever bites. Florida is nice. You like baseball?\n",
      "<other>Yes, I used to live in cincinnati so, I'm a big reds fan.\n",
      "\t<self>Ah, the reds. I'm a fan even though I'm in new york.\n",
      "<other>Yeah! I'm in the process of setting up a work from home office.\n",
      "\t<self>Sorry, my lasagna was finished in the oven. What kind of vehicle do you have?\n",
      "<other>A hyundai elantra. Good gas mileage. I really like it.\n",
      "\t<self>I gotta have the gas guzzling trucks!\n",
      "<other>My fiance has a ford f150 and it eats the gas!\n",
      "\t<self>Oh, congrats on the engagement. When's the date?\n",
      "<other>It's a \"forever engagement\". We've both been married before.\n",
      "Summary: \n",
      "\tI love  fishing.\n",
      "\tLives in Florida.\n",
      "\tI also lives in Cincinnati.\n",
      "\tI like  baseball and a big Reds fan.\n",
      "\tI am going to start working ffrom home.\n",
      "\tDrives a Hyundai Elantra and likes it.\n",
      "\tI am getting  married.\n",
      "\tI have previously been married.\n",
      "Prediction: \n",
      "\tI love to fish.\n",
      "\tI live in Florida.\n",
      "\tI used to live in Cincinnati. I like baseball.\n",
      "\tI work from home.\n",
      "\tI have a Hyundai Elantra.\n",
      "\tI have a fiance.\n",
      "\tI am engaged.\n",
      "\n",
      "2023-09-02 20:29:15,578 SPAM     | Generate: pred_fact=tensor([ True,  True,  True,  True,  True, False])\n",
      "2023-09-02 20:29:15,579 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,    33, 13137,     4,     2,     1,     1],\n",
      "        [    2,     0,   100,   697,    11,  2293,     4,     2,     1],\n",
      "        [    2,     0,   100,   524,    10, 26565,     4,     2,     1],\n",
      "        [    2,     0,   100,   173,    13,   501,   107,     4,     2],\n",
      "        [    2,     0,  2387,  2674,  4204,    16,  1637,     4,     2],\n",
      "        [    2,     0,     2,     1,     1,     1,     1,     1,     1]])\n",
      "Utterances: \n",
      "\t<self>Hey, how are you?\n",
      "<other>Good! My sister and I are twins. Do you have any siblings?\n",
      "\t<self>Yes, but we aren't twins. We do both love baseball though.\n",
      "<other>So do I. I love the twins as we live in mn..\n",
      "\t<self>Gotcha, we're red sox fans cuz we live in new england.\n",
      "<other>What do you do? I am a dentist.\n",
      "\t<self>Nice! I'm a truck driver.\n",
      "<other>Tough job. Been doing it long? I've been at my work for 14 years.\n",
      "\t<self>Yeah, I've been doing it for a while. Sometimes I fish for extra money too.\n",
      "<other>Invest in gold. That's my favorite metal for investment.\n",
      "\t<self>Nice. I've got some bitcoin, but no gold.\n",
      "<other>That's good, too. We'll see how they do...\n",
      "Summary: \n",
      "\tI have a twin sister.\n",
      "\tI live in Minnesota.\n",
      "\tI love the Minnesota Twins.\n",
      "\tI am a dentist.\n",
      "\tI've been a dentist for 14 years.\n",
      "\tMy favorite investment is in gold.\n",
      "Prediction: \n",
      "\tI have twins.\n",
      "\tI live in Michigan.\n",
      "\tI am a dentist.\n",
      "\tI work for 14 years.\n",
      "\tMy favorite metal is gold.\n",
      "\n",
      "2023-09-02 20:29:19,074 SPAM     | Generate: pred_fact=tensor([ True,  True,  True,  True, False, False])\n",
      "2023-09-02 20:29:19,075 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,   173,   411,   360,    10,   186,     4,     2,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,     0,   100,   657,   909,   579,  8448,  2681,     4,     2,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,     0,   100,   657, 31599,  7150,   268,     4,     2,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,     0,   100,  1930,   350,   203,   418,    15, 12858,     8,\n",
      "         31599,  7150,   268,     4,     2],\n",
      "        [    2,     0,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,     0,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "Utterances: \n",
      "\t<self>Hello, do you have any time off or holidays coming up?\n",
      "<other>I wish. I have to work six days, at least 12 hours a day for the next month, maybe more.\n",
      "\t<self>I will be traveling over the easter weekend. I got a new dress and everything.\n",
      "<other>Cool! Sounds like you are ready for a concert. I love black sabbath.\n",
      "\t<self>I like all kinds of music that most people my age like, I'm twenty eight.\n",
      "<other>Are you a vegetarian? I love hamburgers too much to give them up.\n",
      "\t<self>I don't follow any special type of diet.\n",
      "<other>I have been spending too much money on concerts and hamburgers. Also, consumed with work.\n",
      "\t<self>I really enjoy traveling and am looking forward to putting miles on my new car.\n",
      "<other>Where are you traveling for the easter holiday?\n",
      "\t<self>I think my mom will faint when she sees my blue hair.\n",
      "<other>Nice color, but where are you going for easter?\n",
      "Summary: \n",
      "\tI love black sabbath.\n",
      "\tI love hamburgers.\n",
      "\tI have a job.\n",
      "Prediction: \n",
      "\tI work six days a week.\n",
      "\tI love black sabbath.\n",
      "\tI love hamburgers.\n",
      "\tI spend too much money on concerts and hamburgers.\n",
      "\n",
      "2023-09-02 20:29:22,191 SPAM     | Generate: pred_fact=tensor([False,  True,  True,  True,  True,  True, False])\n",
      "2023-09-02 20:29:22,192 SPAM     | Generate: gen_out=tensor([[    2,     0,     2,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,     0,   100,    33,    10,  2793,   512,     4,     2,     1],\n",
      "        [    2,     0,   100,   697,    11,   886,     4,     2,     1,     1],\n",
      "        [    2,     0,   100,   697,    11,  7227,     4,     2,     1,     1],\n",
      "        [    2,     0,  2387,  1041,    58, 28530,     4,     2,     1,     1],\n",
      "        [    2,     0,   100,   524, 12953,    19,  2440,  2473,     4,     2],\n",
      "        [    2,     0,     2,     1,     1,     1,     1,     1,     1,     1]])\n",
      "Utterances: \n",
      "\t<self>Hello! How are you?\n",
      "<other>Hi! Im well, you?\n",
      "\t<self>Great. Just got home from where I work as a car salesman.\n",
      "<other>Oh neat. I have a smart car. Its small but I love it! Where are you from?\n",
      "\t<self>I am from california! In my spare time I enjoy gardening.\n",
      "<other>Id love to live in ca by the beach when I retire. We're from up north.\n",
      "\t<self>Do you like to read? I love mystery novels.\n",
      "<other>I started reading a lot while I lived in alaska. It was a long 3 years.\n",
      "\t<self>I think I like them because my mom was a police officer.\n",
      "<other>That makes sense. My parents were vikings. Like I said, they were northerners!\n",
      "\t<self>Thats great! What kind of music do you like? My fav is 5 finger death punch.\n",
      "<other>Im blonde with blue eyes so I look sweet but I love to rock out! Nin \\m/.\n",
      "\t<self>Thats great! My hair is brown and I have brown eyes.\n",
      "<other>You sound like a really fun person. Id go to a show with you!\n",
      "Summary: \n",
      "\tI have a smart car.\n",
      "\tI am from up north.\n",
      "\tI would like to retire in CA by the beach.\n",
      "\tI read a lot.\n",
      "\tI lived in Alaska for 3 years.\n",
      "\tMy parents were Vikings.\n",
      "\tI like rock.\n",
      "\tI have blond hair and blue eyes.\n",
      "\tI like to go to shows.\n",
      "Prediction: \n",
      "\tI have a smart car.\n",
      "\tI live in California.\n",
      "\tI live in Alaska.\n",
      "\tMy parents were Viking.\n",
      "\tI am blonde with blue eyes.\n",
      "\n",
      "2023-09-02 20:29:22,681 DEBUG    | calc_stats 2: update 12 combinations\n",
      "2023-09-02 20:29:22,683 DEBUG    | calc_stats 32: update 64 combinations\n",
      "2023-09-02 20:29:22,691 DEBUG    | calc_stats 328: update 30 combinations\n",
      "2023-09-02 20:29:22,695 DEBUG    | calc_stats 20: update 12 combinations\n",
      "2023-09-02 20:29:22,698 DEBUG    | calc_stats 494: update 45 combinations\n",
      "2023-09-02 20:29:22,704 DEBUG    | calc_stats 2: compute ['ter']\n",
      "2023-09-02 20:29:22,705 DEBUG    | calc_stats 32: compute ['ter']\n",
      "2023-09-02 20:29:22,706 DEBUG    | calc_stats 328: compute ['ter']\n",
      "2023-09-02 20:29:22,708 DEBUG    | calc_stats 20: compute ['ter']\n",
      "2023-09-02 20:29:22,709 DEBUG    | calc_stats 494: compute ['ter']\n"
     ]
    }
   ],
   "source": [
    "eval_kwargs = {'metrics': ['ter'], 'device': 'cpu', 'log_interval': 10, \"generation_config\": {\"max_new_tokens\": 20}}\n",
    "eval_stats = msc_summaries.evaluate(model, **eval_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   100,   213,    89,     2,     1,     1],\n",
      "        [    0,   100,   213, 50118,  8585,     2,     1],\n",
      "        [    0,   100,   213, 50118,    89,     2,     1],\n",
      "        [    0,   100,   213,  1437, 50118,  8585,     2],\n",
      "        [    0,   100,   213,  1437, 50118,    89,     2],\n",
      "        [    0, 41552,   282,  1116,  7257, 15698,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]])}\n",
      "['<s>', 'I', 'Ġgo', 'Ġthere', '</s>', '<pad>', '<pad>']\n",
      "['<s>', 'I', 'Ġgo', 'Ċ', 'there', '</s>', '<pad>']\n",
      "['<s>', 'I', 'Ġgo', 'Ċ', 'Ġthere', '</s>', '<pad>']\n",
      "['<s>', 'I', 'Ġgo', 'Ġ', 'Ċ', 'there', '</s>']\n",
      "['<s>', 'I', 'Ġgo', 'Ġ', 'Ċ', 'Ġthere', '</s>']\n",
      "['<s>', '<', 'n', 'of', 'act', '>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"I go there\"\n",
    "s2 = \"I go\\nthere\"\n",
    "s3 = \"I go\\n there\"\n",
    "s4 = \"I go \\nthere\"\n",
    "s5 = \"I go \\n there\"\n",
    "s6 = \"<nofact>\"\n",
    "encoded_utterances = tokenizer(text=[s1, s2, s3, s4, s5, s6], return_tensors='pt', padding=True)\n",
    "print(encoded_utterances)\n",
    "for enc in encoded_utterances['input_ids']:\n",
    "    print(tokenizer.convert_ids_to_tokens(enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   100,   213,    89, 50118,  7608,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,   100,   213,    89,  2612,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0, 41552, 13367, 15698,   100,   213,    89, 28696,  7443, 15698,\n",
      "          7608,     2,     1,     1],\n",
      "        [    0, 41552, 13367, 15698,   100,   213,    89, 50118, 41552,  7443,\n",
      "         15698,  7608,     2,     1],\n",
      "        [    0, 41552, 13367, 15698,   100,   213,    89, 50118, 41552,  7443,\n",
      "         15698,  2612,     2,     1],\n",
      "        [    0, 41552, 13367, 15698,    38,   213,    89, 50118, 28696,  7443,\n",
      "         15698,  2612,     2,     1],\n",
      "        [    0, 41552, 13367, 15698,   100,   213,    89,  1437, 50118, 28696,\n",
      "          7443, 15698,  2612,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['<s>', 'I', 'Ġgo', 'Ġthere', 'Ċ', 'Why', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'I', 'Ġgo', 'Ġthere', 'ĠWhy', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<', 'self', '>', 'I', 'Ġgo', 'Ġthere', 'Ġ<', 'other', '>', 'Why', '</s>', '<pad>', '<pad>']\n",
      "['<s>', '<', 'self', '>', 'I', 'Ġgo', 'Ġthere', 'Ċ', '<', 'other', '>', 'Why', '</s>', '<pad>']\n",
      "['<s>', '<', 'self', '>', 'I', 'Ġgo', 'Ġthere', 'Ċ', '<', 'other', '>', 'ĠWhy', '</s>', '<pad>']\n",
      "['<s>', '<', 'self', '>', 'ĠI', 'Ġgo', 'Ġthere', 'Ċ', 'Ġ<', 'other', '>', 'ĠWhy', '</s>', '<pad>']\n",
      "['<s>', '<', 'self', '>', 'I', 'Ġgo', 'Ġthere', 'Ġ', 'Ċ', 'Ġ<', 'other', '>', 'ĠWhy', '</s>']\n"
     ]
    }
   ],
   "source": [
    "s0 = \"I go there\\nWhy\"\n",
    "s0b = \"I go there Why\"\n",
    "s1 = \"<self>I go there <other>Why\"\n",
    "s2 = \"<self>I go there\\n<other>Why\"\n",
    "s3 = \"<self>I go there\\n<other> Why\"\n",
    "s4 = \"<self> I go there\\n <other> Why\"\n",
    "s5 = \"<self>I go there \\n <other> Why\"\n",
    "encoded_utterances = tokenizer(text=[s0, s0b, s1, s2, s3, s4, s5], return_tensors='pt', padding=True)\n",
    "print(encoded_utterances)\n",
    "for enc in encoded_utterances['input_ids']:\n",
    "    print(tokenizer.convert_ids_to_tokens(enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<self>I', 'go', 'there', '<other>Why']\n",
      "['<self>I', 'go', 'there', '<other>Why']\n",
      "['<self>I', 'go', 'there', '<other>', 'Why']\n"
     ]
    }
   ],
   "source": [
    "print(s1.split())\n",
    "print(s2.split())\n",
    "print(s5.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 41552, 13367, 15698,  3945,    47, 15433,    11,     5,   343,\n",
       "            23,    70,    50,   109,    47,   202,   269,  2649,     5,   247,\n",
       "           116, 28696,  7443, 15698,    38,   524, 15433,    11,     6,    53,\n",
       "            38,   269,  2649,    24,     4,     2],\n",
       "        [    0, 41552, 13367, 15698,  3945,    47, 15433,    11,     5,   343,\n",
       "           116, 28696,  7443, 15698,   440,     6,    38,   269,  2649,    24,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"<self> Are you settling in the city at all or do you still really miss the country? <other> I am settling in, but I really miss it.\"\n",
    "s2 = \"<self> Are you settling in the city? <other> No, I really miss it.\"\n",
    "encoded_utterances = tokenizer(text=[s1, s2], return_tensors='pt', padding=True)\n",
    "encoded_utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-02 20:29:23,258 SPAM     | Generate: pred_fact=tensor([True, True])\n",
      "2023-09-02 20:29:23,259 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,   524, 15433,    11,     5,   343,     4,     2],\n",
      "        [    2,     0,   100,   524,  1375,     7,     5,   343,     4,     2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['</s><s>I am settling in the city.</s>',\n",
       " '</s><s>I am moving to the city.</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.cat([encoded_utterances['input_ids'], torch.ones(20, dtype=torch.long).view(2, 10)], dim=1)\n",
    "attn_mask = torch.cat([encoded_utterances['attention_mask'], torch.zeros(20, dtype=torch.long).view(2,10)], dim=1)\n",
    "pred_tokens_2 = model.generate(\n",
    "    input_ids=input_ids.to('cpu'), \n",
    "    # attention_mask=attn_mask.to('cpu'),    # attention_mask is not necessary, is defined within the generatie function\n",
    "    min_length=2,\n",
    "    max_new_tokens=20, \n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    forced_eos_token_id=list(set([tokenizer.eos_token_id, model.nofact_token_id]))\n",
    ")\n",
    "tokenizer.batch_decode(pred_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 41552, 13367, 15698,  3945,    47, 15433,    11,     5,   343,\n",
       "            23,    70,    50,   109,    47,   202,   269,  2649,     5,   247,\n",
       "           116, 28696,  7443, 15698,    38,   524, 15433,    11,     6,    53,\n",
       "            38,   269,  2649,    24,     4,     2],\n",
       "        [    0, 41552, 13367, 15698,  3945,    47, 15433,    11,     5,   343,\n",
       "           116, 28696,  7443, 15698,   440,     6,    38,   269,  2649,    24,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_utterances['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 41552, 13367, 15698,  3945,    47, 15433,    11,     5,   343,\n",
       "            23,    70,    50,   109,    47,   202,   269,  2649,     5,   247,\n",
       "           116, 28696,  7443, 15698,    38,   524, 15433,    11,     6,    53,\n",
       "            38,   269,  2649,    24,     4,     2],\n",
       "        [    0, 41552, 13367, 15698,  3945,    47, 15433,    11,     5,   343,\n",
       "           116, 28696,  7443, 15698,   440,     6,    38,   269,  2649,    24,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-02 20:29:23,811 SPAM     | Generate: pred_fact=tensor([True, True])\n",
      "2023-09-02 20:29:23,811 SPAM     | Generate: gen_out=tensor([[    2,     0,   100,   524, 15433,    11,     5,   343,     4,     2],\n",
      "        [    2,     0,   100,   524,  1375,     7,     5,   343,     4,     2]])\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = model.generate(\n",
    "    input_ids=encoded_utterances['input_ids'].to('cpu'), \n",
    "    attention_mask=encoded_utterances['attention_mask'].to('cpu'),\n",
    "    min_length=2,\n",
    "    max_new_tokens=20, \n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,   100,   524, 15433,    11,     5,   343,     4,     2],\n",
       "        [    2,     0,   100,   524,  1375,     7,     5,   343,     4,     2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s><s>I am settling in the city.</s>',\n",
       " '</s><s>I am moving to the city.</s>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bart.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ExtractedFactLoss(nofact_token_id=nofact_token_id, ignore_index=-100, lm_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randint(0, 100, (2,3,8)).float()\n",
    "t = torch.randint(0,7, (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[67., 35., 31., 28., 79., 76., 92., 83.],\n",
       "          [ 0., 45., 21., 48., 90., 17., 44., 97.],\n",
       "          [12., 38., 22., 20., 90., 50., 72., 65.]],\n",
       " \n",
       "         [[46., 32., 10., 43., 92., 96., 97., 59.],\n",
       "          [52.,  1., 26., 36., 69.,  7.,  2., 58.],\n",
       "          [80., 61., 45., 27., 20., 25., 18., 83.]]]),\n",
       " tensor([[0, 6, 0],\n",
       "         [2, 0, 0]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-67., -44., -12.],\n",
       "        [-10., -52., -80.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.nllloss(p.permute(0,2,1), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor([[[-6, 0, -5, 7], [-.5, -.5, -8, 0]]]).float()\n",
    "t = torch.tensor([[0, -100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion.nllloss.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
