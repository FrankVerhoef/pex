{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.msc_sessions import MSC_Session\n",
    "from dataset.msc_speechact import MSC_SpeechAct\n",
    "from models.speechact_clf import SpeechactClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_options = {\n",
    "    \"speaker_prefixes\": [None, [\"<other>\", \"<self>\"]],\n",
    "}\n",
    "basedir = \"/Users/FrankVerhoef/Programming/PEX/data/msc/msc_dialogue/\"\n",
    "checkpoint_dir = \"/Users/FrankVerhoef/Programming/PEX/checkpoints/\"\n",
    "subsets = {\n",
    "    1: ['train', 'valid', 'test'],\n",
    "    # 2: ['train', 'valid', 'test'],\n",
    "    # 3: ['train', 'valid', 'test'],\n",
    "    # 4: ['train', 'valid', 'test'],\n",
    "    # 5: ['valid', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    \"default\": {\n",
    "        \"speaker_prefixes\": [\"<other>\", \"<self>\"],\n",
    "        \"sessionbreak_token\": \"<sessionbreak>\",\n",
    "        \"speechact_classifier\": None\n",
    "    },\n",
    "    \"speechacts\": {\n",
    "        \"speaker_prefixes\": [\"<other>\", \"<self>\"],\n",
    "        \"sessionbreak_token\": \"<sessionbreak>\",\n",
    "        \"speechact_classifier\": SpeechactClassifier(checkpoint_dir=checkpoint_dir, modelname=\"trained_speechact_bert\")\n",
    "    }\n",
    "}\n",
    "\n",
    "variants = {\n",
    "    \"no_persona_no_hist\": {\"include_persona\": False, \"include_history\": False},\n",
    "    # \"persona_no_hist\": {\"include_persona\": True, \"include_history\": False},\n",
    "    # \"persona_and_hist\": {\"include_persona\": True, \"include_history\": True},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSC_Session.set(**configs['default'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-03 00:51:46,603 INFO     | Init ConvAI2 with basedir=/Users/FrankVerhoef/Programming/PEX/data/msc/msc_dialogue/ConvAI2/, version=['both', 'revised'], subset=train\n",
      "2023-08-03 00:51:46,603 INFO     | For ConvAI2 dataset, use 90% of train dataset for training (rest is available as validation dataset)\n",
      "2023-08-03 00:51:47,425 INFO     | Read 16090 dialogues from ConvAI2 for train dataset\n",
      "2023-08-03 00:51:47,429 INFO     | Init ConvAI2 with basedir=/Users/FrankVerhoef/Programming/PEX/data/msc/msc_dialogue/ConvAI2/, version=['both', 'revised'], subset=valid\n",
      "2023-08-03 00:51:47,430 INFO     | For ConvAI2 dataset, use 10% of train dataset as validation dataset\n",
      "2023-08-03 00:51:48,135 INFO     | Read 1788 dialogues from ConvAI2 for valid dataset\n",
      "2023-08-03 00:51:48,138 INFO     | Init ConvAI2 with basedir=/Users/FrankVerhoef/Programming/PEX/data/msc/msc_dialogue/ConvAI2/, version=['both', 'revised'], subset=test\n",
      "2023-08-03 00:51:48,139 INFO     | For ConvAI2 dataset, use validation dataset as test dataset\n",
      "2023-08-03 00:51:48,170 INFO     | Read 1000 dialogues from ConvAI2 for test dataset\n"
     ]
    }
   ],
   "source": [
    "max_samples = 100\n",
    "msc_sessions = {}\n",
    "for session in subsets.keys():\n",
    "    if session == 1:\n",
    "        version = ['both', 'revised']\n",
    "        session = '-'.join(['1'] + version)\n",
    "    msc_sessions[int(str(session)[0])] = {}\n",
    "    for option_name in variants.keys():\n",
    "        msc_sessions[int(str(session)[0])][option_name] = {\n",
    "            subset: MSC_Session(basedir=basedir, session=session, subset=subset, max_samples=max_samples, **variants[option_name]) \n",
    "            for subset in subsets[int(str(session)[0])]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSC_Session.speechact_classifier is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {\n",
    "    session: {\n",
    "        option_name: {subset: msc_sessions[session][option_name][subset].measurements() for subset in subsets[session]}\n",
    "        for option_name in variants.keys()\n",
    "    }\n",
    "    for session in subsets.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('speechact_test_s3_100.json', 'r') as f:\n",
    "    # f.write(json.dumps(m))\n",
    "    m = {3: json.loads(f.read())['3']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<sessionbreak>new session\\n<other>hi how are you this evening ?\\n<self>i am good , are you into music\\n<other>do you have any favorite music artists ? mine is drake .\\n<self>insane clown posse , i love there shows\\n<other>i also enjoy other rap artists too . i like them also .\\n<self>i love all rap , do you do other stuff\\n<other>yes , cooking for family get togethers is one of my favorites\\n<self>i like to cook a little i am really into larp\\n<other>i play the drums every now and again too . what is larp ?\\n<self>live action role playing it awesome\\n<other>sounds like it could be a good time .\\n<self>we dress up in armor and fight with swords and other weapons\\n<other>cool , i play the drums every now and again\\n', '<self>what kind of work do you do ?\\n')\n",
      "(\"<sessionbreak>new session\\n<other>hey how are you today\\n<self>i am good . what about you ?\\n<other>i'm good , just doing some weight lifting this morning\\n<self>cool . my kids got me up early wanting breakfast .\\n<other>ahahah my daughter just turned one , so i made her some eggs this morning .\\n<self>yum ! we are planning to go swimming and do some online shopping today . what about you ?\\n<other>i'm doing training because i'm becoming a policeman soon .\\n<self>wow ! sounds great ! i stay home with my kids , but i worked for monsanto before .\\n<other>oh cool , that's always great to be with your kids ,\\n<self>yes , i enjoy it . i also like to read . what else do you do ?\\n<other>i watch a ton of mma\\n<self>i see . i like fiction books . mostly fantasy .\\n<other>do you like collecting knives\\n\", \"<self>no , not really . i would be afraid my kids would get them when i wasn't looking .\\n\")\n",
      "(\"<sessionbreak>new session\\n<other>good morning ! just finished my first ebook yay ! what are you up to ?\\n<self>waiting to get off an go workout !\\n<other>workouts are awesome i do sumba and love it , do you weight lift ?\\n<self>yes i do i did legs yesterday man ! got a date tonight !\\n<other>abs are what matters there are reasons why superheros make the best shows\\n<self>lol i like to cook !\\n<other>well , eat right , and then the belly fat will not come on\\n<self>i'm glad i do what i love as a extra i do not work\\n<other>really ! what do you do ?\\n<self>i want to drink tho i love meeting women at the bar\\n<other>you can go to a bar without drinking you know but you might want one though .\\n<self>ll very true ! i'm cooking a healthy meal\\n<other>you a bit of a health nut then i take it ?\\n\", '<self>yes i am , i am a trainer\\n')\n",
      "(\"<sessionbreak>new session\\n<other>whats up ! ! ! i love pizza so much\\n<self>me not so much . i eat a few bites then toss it to my border collies .\\n<other>oh . . . dogs . i cannot say that i like animals . . at all\\n<self>i love em . i've mollie and tarter sauce and i take them hiking with me .\\n<other>i'll just role play that i went hiking lol\\n<self>you role play for therapy or pleasure ?\\n<other>just for fun , sometimes i pretend i am a slice of pizza\\n<self>oh another performance artist , just like me ! where you live ?\\n<other>i love all sorts of cars , so i just travel the us living in cars\\n<self>what does that have to do with where you live ?\\n<other>i do not live anywhere ? i'm a nomad\\n<self>oh my ! i'm going to move from tx to az next month and its freaking me out !\\n<other>i like traveling in the warm states\\n\", '<self>i like cold winters actually . its getting late here so bonne nuit !\\n')\n",
      "('<sessionbreak>new session\\n<other>hi ! how are you today ?\\n<self>i am doing great ! i just wrote a new poem .\\n<other>awesome ! what was the poem about ?\\n<self>how much i love my pet beta fish .\\n<other>those are so cool , i love fish . i love all animals , actually .\\n<self>what is your favorite animal ? i have always wanted a pet alligator .\\n<other>awesome ! i love dogs but i have always wanted a pet pig .\\n<self>pigs are really cool . i would love to take it golfing with my friends\\n<other>too funny ! i would teach it to dance to country music , my favorite .\\n<self>oh all the members of the gun club i head would love that !\\n<other>i bet they would , it would be quite a sight !\\n<self>we could dress it up in the old clothes that we collected for the homeless .\\n<other>yes ! we could make him the cover model for my vegan newsletter .\\n<self>that would be totally awesome !\\n<other>yes it would be great .\\n', '<self>maybe i will go to the zoo tomorrow to pick one up .\\n')\n",
      "(\"<sessionbreak>new session\\n<other>how are you this fine evening ?\\n<self>well i am doing quite lovely . just wishing i could goto a justin bieber concert .\\n<other>oh you have the bieber fever do you ?\\n<self>yes ! ! ! ! i often wish i could quit my job and just follow him around ! !\\n<other>lol , what kind of work do you do ?\\n<self>i'm a professional wedding photographer\\n<other>cool fall is a great time of year for pictures here in new york .\\n<self>yea , i love watching the leaves change color . what do you do ?\\n<other>i am a grad student . hoping to get into law school next year .\\n<self>cool ! i've 5 pairs of the same pants\\n<other>really ? you must really hate doing laundry ! lol\\n<self>i hate choosing what to wear ! !\\n<other>so where are you from ?\\n<self>i live in colorado . i was born in arizona\\n<other>have you ever been to ny ?\\n\", '<self>yes , i was just there for vacation . such a busy place . never knew people honked so much\\n')\n",
      "(\"<sessionbreak>new session\\n<other>good evening . hows your day going ?\\n<self>my day is fine , been peaceful .\\n<other>glad to hear . been listening to music all day , keeps me at peace as well .\\n<self>mine comes from reading and writing . i think i get it from my mom , the librarian .\\n<other>that make sense . love my mom , she keeps me grounded !\\n<self>they're great . so , what do you do for a living ?\\n<other>i want to be a musician . following a tight schedule to try to make it happen .\\n<self>that's awesome ! i hope you make it .\\n<other>thanks ! i am confident i will someday . for now i will just binge greys anatomy lol\\n<self>i have been spending most of my time lately applying to publishing companies .\\n<other>for a writing position or as a copy editor or something ?\\n\", \"<self>i'm truly hoping for a writing position , but anything related is fine by me .\\n\")\n",
      "(\"<sessionbreak>new session\\n<other>how are you doing today\\n<self>good just got home from jogging .\\n<other>do you like to hunt\\n<self>no , but i give blood ! every single month . it is for my sister who passed .\\n<other>what happened to her ? i bet that was hard .\\n<self>she was hit by a car . the hospital didn't have enough blood .\\n<other>that's just way crazy\\n<self>it is , so i donate to make sure i do my part .\\n<other>that's a very nice thing to do\\n<self>thanks i just do not want another to feel helpless .\\n<other>that's so great . you help so many people .\\n<self>thank you , if only i could have helped my sister .\\n<other>i am sorry for your loss\\n\", '<self>thank you , have any fun hobbies ?\\n')\n",
      "(\"<sessionbreak>new session\\n<other>hi there ! how are you ?\\n<self>i'm just reading a book\\n<other>what book ? i hate reading\\n<self>the notebook , a reread lol\\n<other>books get in the way of my video games\\n<self>i guess . how many phones do you have\\n<other>one . do you have more than one ?\\n<self>i've one plus one .\\n<other>that equals two my friend\\n<self>yep , that's how many i have . i can call you on one of them\\n<other>that sounds fun ! we can listen to country music !\\n<self>i mean . i guess if that's what you want to do\\n<other>or you could cover my shift at subway\\n\", \"<self>that would work , i'm hungry\\n\")\n",
      "(\"<sessionbreak>new session\\n<other>hi , how are you today ?\\n<self>hey there , wanna go shopping ?\\n<other>i am leaving right now to go pick up my new car .\\n<self>hey what kind ? i am studying to get my degree .\\n<other>i am getting ready to head to college . how long have you been studying ?\\n<self>seems like a lifetime ! trying to be a nurse .\\n<other>that's admirable . i'm going to work on a business degree .\\n<self>cool , i'm headed out for a little drive its nice out .\\n<other>i am taking my new car and going to florida state for a tour .\\n<self>you are crazy ! irma ! i live in a tiny town .\\n<other>did you get loans for your degree ?\\n<self>yes i did a few , why ?\\n<other>yes , well they didn't cancel the tour . hoping to get it over quick .\\n\", \"<self>i pray for those that didn't make it .\\n\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(msc_sessions[1]['no_persona_no_hist']['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(msc_sessions[2]['persona_and_hist']['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(msc_sessions[4]['persona_and_hist']['valid'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speechacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [3]\n",
    "variant = \"no_persona_no_hist\"\n",
    "subset = 'train'\n",
    "shuffled_dialogues = [\n",
    "    (msc_sessions[3][variant]['train'].indices[i], msc_sessions[3][variant]['train'].history[i])\n",
    "    for i in np.random.permutation(len(msc_sessions[3][variant]['train']))]\n",
    "ref_stats, ref_selfchats_results = MSC_Session.calc_speechact_stats(shuffled_dialogues)\n",
    "ref_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/FrankVerhoef/Programming/PEX/notebooks/speechact_test_s3_100.json'\n",
    "with open (filename, 'r') as f:\n",
    "    m = {3: json.loads(f.read())['3']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.keys())\n",
    "print(m[3].keys())\n",
    "print(m[3]['no_persona_no_hist'].keys())\n",
    "Counter(m[3][variant][subset]['speechacts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency, wasserstein_distance\n",
    "\n",
    "def chi_squared(observed, floor=0):\n",
    "    # remove rows with 0 sum\n",
    "    filtered = observed[observed.sum(axis=1) > max(floor, 0)]\n",
    "    # print(f\"Filtered: {1 - np.sum(filtered) / np.sum(observed):.2%}\")\n",
    "    return chi2_contingency(filtered)\n",
    "\n",
    "def normalize(counter):\n",
    "    total = sum(counter.values())\n",
    "    normalized = {k: v / total for k, v in counter.items()}\n",
    "    return normalized\n",
    "\n",
    "def speechact_sim(speechacts_1, speechacts_2):\n",
    "    \"\"\"\n",
    "    Calculates a similarity score between two dialogues based on the probability of the speechacts in each item.\n",
    "    Based on averag of squared difference between probabilities of speechacts\n",
    "    \"\"\"\n",
    "    dist_1 = normalize(speechacts_1)\n",
    "    dist_2 = normalize(speechacts_2)\n",
    "    diff = np.array([dist_1.get(speechact, 0) - dist_2.get(speechact, 0) for speechact in MSC_SpeechAct.classes.keys()])\n",
    "    sim = sum(diff * diff) / len(speechact_keys)\n",
    "    return sim\n",
    "\n",
    "observations = np.array([\n",
    "    [13, 15],\n",
    "    [30, 13],\n",
    "    [44, 65]\n",
    "])\n",
    "\n",
    "print(chi_squared(observations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-squared test applied to frequency of speechacts\n",
    "\n",
    "sorted_acts = sorted(sum([Counter(m[s][variant][subset]['speechacts']) for s in sessions for subset in ['train', 'valid', 'test']], Counter()).keys())\n",
    "x_acts = np.arange(len(sorted_acts))\n",
    "\n",
    "data = np.array([\n",
    "    [m[3][variant][subset][\"speechacts\"].get(k, 0) for k in sorted_acts]\n",
    "    for subset in ['train', 'valid', 'test']\n",
    "])\n",
    "\n",
    "chi_tt = chi_squared(data[np.array([0,1])].T, floor=0)\n",
    "chi_tv = chi_squared(data[np.array([0,2])].T, floor=0)\n",
    "chi_vt = chi_squared(data[np.array([1,2])].T, floor=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "for i, (s, subset) in enumerate(itertools.product(sessions, ['train', 'valid', 'test'])):\n",
    "    subset_m = m[s][variant][subset]\n",
    "    offset = 0.2 * i - 0.2\n",
    "    ax.bar(x_acts + offset, height=data[i], width=0.2, label=f\"{s}-{subset}\")\n",
    "    # ax.bar(x_acts + offset, height=[subset_m[\"speechacts\"].get(k, 0) for k in sorted_acts], width=0.2, label=f\"{s}-{subset}\")\n",
    "\n",
    "ax.set_xticks(x_acts)\n",
    "ax.set_xticklabels([MSC_SpeechAct.classes[c] for c in sorted_acts], rotation=0)\n",
    "ax.set_xlabel(\"Speech acts\")\n",
    "ax.legend()\n",
    "\n",
    "fig.suptitle(f\"Frequency of speech acts in sample of 100 dialogues\\n$\\chi^2$ \"\n",
    "    f\"train-valid: {chi_tv.statistic:.1f} (p={chi_tv.pvalue:.2f}), \"\n",
    "    f\"train-test: {chi_tt.statistic:.1f} (p={chi_tt.pvalue:.2f}), \"\n",
    "    f\"valid-test: {chi_vt.statistic:.1f} (p={chi_vt.pvalue:.2f}), \"\n",
    ")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wasserstein Distance calculated between probabilities of speechacts per dialogue\n",
    "\n",
    "for subset in ['train', 'valid', 'test']:\n",
    "    all_measurements = m[3]['no_persona_no_hist'][subset]['allitem_measurements']\n",
    "    for measurements in all_measurements:\n",
    "        measurements['speechacts_normalized'] = normalize(measurements['speechacts'])\n",
    "\n",
    "speechact_keys = MSC_SpeechAct.classes.keys()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(speechacts), figsize=(9,3 * len(speechacts)), sharex=True)\n",
    "num_bins = 20\n",
    "bins = np.linspace(0, 1, num_bins+1)\n",
    "x = np.arange(num_bins)\n",
    "\n",
    "for speechact, ax in zip(speechact_keys, axs):\n",
    "    data = {\n",
    "        subset: np.array([\n",
    "            measurements['speechacts_normalized'][speechact] \n",
    "            for measurements in m[3]['no_persona_no_hist'][subset]['allitem_measurements']\n",
    "            if speechact in measurements['speechacts_normalized'].keys()\n",
    "            ])\n",
    "        for subset in ['train', 'valid', 'test']\n",
    "    }\n",
    "    for i, subset in enumerate(['train', 'valid', 'test']):\n",
    "        hist = np.histogram(data[subset], bins=bins)\n",
    "        # print(hist)\n",
    "        offset = 0.2 * i + 0.3\n",
    "        ax.bar(x + offset, height=hist[0]/100, width=0.2, label=f\"{3}-{subset}\")\n",
    "\n",
    "\n",
    "    wd_tv = wasserstein_distance(data['train'], data['valid']) if len(data['train']) > 0 else -1\n",
    "    wd_tt = wasserstein_distance(data['train'], data['test'])if len(data['train']) > 0 else -1\n",
    "    wd_vt = wasserstein_distance(data['valid'], data['test'])if len(data['valid']) > 0 else -1\n",
    "\n",
    "    ax.set_title(f\"Speechact {speechact}: WD(train-valid)={wd_tv:.4f}, WD(train-test)={wd_tt:.4f}, WD(valid-test)={wd_vt:.4f}\")\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, num_bins)\n",
    "    ax.set_xticks(np.arange(num_bins + 1))\n",
    "    ax.set_xticklabels([f\"{b:.2f}\" for b in bins])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speechact_sim(speechacts_1, speechacts_2):\n",
    "\n",
    "    dist_1 = normalize(speechacts_1)\n",
    "    dist_2 = normalize(speechacts_2)\n",
    "    diff = np.array([dist_1.get(speechact, 0) - dist_2.get(speechact, 0) for speechact in MSC_SpeechAct.classes.keys()])\n",
    "    sim = sum(diff * diff) / len(speechact_keys)\n",
    "\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to visualize similarity between dialogues on item level\n",
    "# (but this does not really make sense)\n",
    "\n",
    "for subset in ['train', 'valid', 'test']:\n",
    "    all_measurements = m[3]['no_persona_no_hist'][subset]['allitem_measurements']\n",
    "    for measurements in all_measurements:\n",
    "        measurements['speechacts_normalized'] = normalize(measurements['speechacts'])\n",
    "\n",
    "speechact_keys = ['A', 'E'] #, 'P', 'Q', 'R', 'S']\n",
    "comparisons = [['train', 'valid'], ['train', 'test'], ['valid', 'test']]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(speechact_keys), ncols=len(comparisons), figsize=(3 * len(comparisons),3 * len(speechact_keys)), sharex=True, sharey=True)\n",
    "\n",
    "num_bins = 20\n",
    "num_samples = 100\n",
    "bins = np.linspace(0, 1, num_bins+1)\n",
    "x = np.arange(num_bins)\n",
    "\n",
    "for (speechact, comparison), ax in zip(itertools.product(speechact_keys, comparisons), axs.flatten()):\n",
    "\n",
    "    # Take random samples of two sets and calculate similarity\n",
    "    print(speechact, comparison)\n",
    "    list_1 = [item_m['speechacts'] for item_m in m[3]['no_persona_no_hist'][comparison[0]]['allitem_measurements']]\n",
    "    list_2 = [item_m['speechacts'] for item_m in m[3]['no_persona_no_hist'][comparison[1]]['allitem_measurements']]\n",
    "    data = np.array([\n",
    "        speechact_sim(item_1, item_2)\n",
    "        for item_1, item_2 in zip(random.choices(list_1, k=num_samples), random.choices(list_2, k=num_samples))\n",
    "    ])\n",
    "    # print(data)\n",
    "    hist = np.histogram(data, bins=bins)\n",
    "    print(hist)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements overview with Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>inputwords</th>\n",
       "      <th>inputsentences</th>\n",
       "      <th>labelwords</th>\n",
       "      <th>ref_self</th>\n",
       "      <th>ref_other</th>\n",
       "      <th>ref_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.0</td>\n",
       "      <td>894.026667</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>264.126667</td>\n",
       "      <td>11.730000</td>\n",
       "      <td>24.390000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1141.881543</td>\n",
       "      <td>0.941659</td>\n",
       "      <td>79.243226</td>\n",
       "      <td>0.941659</td>\n",
       "      <td>12.769551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>184.750000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3966.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>578.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       session    dialog_id     turn_id  inputwords  inputsentences  \\\n",
       "count    300.0   300.000000  300.000000  300.000000      300.000000   \n",
       "mean       3.0   894.026667   10.730000  264.126667       11.730000   \n",
       "std        0.0  1141.881543    0.941659   79.243226        0.941659   \n",
       "min        3.0     6.000000    7.000000  103.000000        8.000000   \n",
       "25%        3.0   184.750000   11.000000  206.000000       12.000000   \n",
       "50%        3.0   360.000000   11.000000  255.000000       12.000000   \n",
       "75%        3.0   992.000000   11.000000  303.000000       12.000000   \n",
       "max        3.0  3966.000000   11.000000  578.000000       12.000000   \n",
       "\n",
       "       labelwords  ref_self  ref_other  ref_context  \n",
       "count  300.000000     300.0      300.0   300.000000  \n",
       "mean    24.390000       0.0        0.0     0.600000  \n",
       "std     12.769551       0.0        0.0     0.490716  \n",
       "min      5.000000       0.0        0.0     0.000000  \n",
       "25%     15.750000       0.0        0.0     0.000000  \n",
       "50%     22.000000       0.0        0.0     1.000000  \n",
       "75%     30.000000       0.0        0.0     1.000000  \n",
       "max     90.000000       0.0        0.0     1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect all item measurements in a dataframe\n",
    "session = 3\n",
    "\n",
    "df_colums = list(m[session][\"no_persona_no_hist\"]['train'][\"allitem_measurements\"][0].keys())\n",
    "df = pd.DataFrame(columns=df_colums).astype('int16')\n",
    "\n",
    "for session in subsets.keys():\n",
    "    for variant in variants.keys():\n",
    "        for subset in subsets[session]:\n",
    "            subset_df = pd.DataFrame.from_dict(m[session][variant][subset][\"allitem_measurements\"])\n",
    "\n",
    "            subset_df[\"session\"] = int(session)\n",
    "            subset_df[\"variant\"] = variant\n",
    "            subset_df[\"subset\"] = subset\n",
    "        \n",
    "            df = pd.concat([df, subset_df])\n",
    "\n",
    "df[\"session\"] = df[\"session\"].astype('int')\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>dialog_id</th>\n",
       "      <th>turn_id</th>\n",
       "      <th>convai_id</th>\n",
       "      <th>inputwords</th>\n",
       "      <th>inputsentences</th>\n",
       "      <th>labelwords</th>\n",
       "      <th>ref_self</th>\n",
       "      <th>ref_other</th>\n",
       "      <th>ref_context</th>\n",
       "      <th>variant</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>train:ordered_4680</td>\n",
       "      <td>292</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no_persona_no_hist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>11</td>\n",
       "      <td>train:ordered_770</td>\n",
       "      <td>234</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_persona_no_hist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>11</td>\n",
       "      <td>train:ordered_230</td>\n",
       "      <td>272</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_persona_no_hist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>218</td>\n",
       "      <td>11</td>\n",
       "      <td>train:ordered_7371</td>\n",
       "      <td>307</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no_persona_no_hist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>231</td>\n",
       "      <td>11</td>\n",
       "      <td>train:ordered_3986</td>\n",
       "      <td>142</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no_persona_no_hist</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  dialog_id  turn_id           convai_id  inputwords  \\\n",
       "0        3         56        7  train:ordered_4680         292   \n",
       "1        3         99       11   train:ordered_770         234   \n",
       "2        3        122       11   train:ordered_230         272   \n",
       "3        3        218       11  train:ordered_7371         307   \n",
       "4        3        231       11  train:ordered_3986         142   \n",
       "\n",
       "   inputsentences  labelwords  ref_self  ref_other  ref_context  \\\n",
       "0               8          25         0          0            1   \n",
       "1              12          20         0          0            0   \n",
       "2              12          13         0          0            0   \n",
       "3              12          34         0          0            1   \n",
       "4              12          18         0          0            0   \n",
       "\n",
       "              variant subset  \n",
       "0  no_persona_no_hist  train  \n",
       "1  no_persona_no_hist  train  \n",
       "2  no_persona_no_hist  train  \n",
       "3  no_persona_no_hist  train  \n",
       "4  no_persona_no_hist  train  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of statistics, by session and subset\n",
    "\n",
    "df.groupby([\"session\", \"variant\", \"subset\"]).agg({'turn_id': ['count'], 'inputwords': ['mean', 'std'], 'inputsentences': ['mean', 'std'], 'labelwords': ['mean', 'std'], })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=5, figsize=(12, 12))\n",
    "\n",
    "def plot_hist_bar(ax, values, session, title, bins, range):\n",
    "\n",
    "    if session==5:\n",
    "        label = '-' # add empty label\n",
    "        vals = [-1]  # plot a bar, outside the range\n",
    "        ax.hist(vals, bins=bins, range=range, alpha=0.5, label=label, density=True)\n",
    "    for subset, vals in zip(subsets[session], values): \n",
    "        label = f\"{subset}: \" + r\"$\\mu$\" + f\"={vals.mean():.0f}, \" + r\"$\\sigma$\" + f\"={vals.std():.0f}\"\n",
    "        ax.hist(vals, bins=bins, range=range, alpha=0.5, label=label, density=True)\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "for j, variant in enumerate(variants.keys()):\n",
    "    for i, session in enumerate(subsets.keys()):\n",
    "        m_input = []\n",
    "        for subset in subsets[session]:\n",
    "            selection = selection = (df[\"session\"] == session) & (df[\"variant\"] == variant) & (df[\"subset\"] == subset)\n",
    "            m_input.append(df[selection]['inputwords'].values)\n",
    "        bar_axes = plot_hist_bar(ax[i][j], m_input, session, title=f\"Session_{session}, {variant}\\nn={len(m_input[0])}\", bins=40, range=(0,2000))\n",
    "\n",
    "fig.suptitle(f\"Distribution of number of inputwords (persona sentences, history, current dialogue)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 4), sharey=False, sharex=True)\n",
    "\n",
    "def plot_mean_with_std(ax, values, subset):\n",
    "    sessions = [s for s in subsets.keys() if subset in subsets[s]]\n",
    "    means = np.array([vals.mean() for vals in values])\n",
    "    stds = np.array([vals.std() for vals in values])\n",
    "    ax.plot(sessions, means, lw=2, label=subset)\n",
    "    ax.fill_between(sessions, means+stds, means-stds, alpha=0.2)\n",
    "    return ax\n",
    "\n",
    "for j, variant in enumerate(variants.keys()):\n",
    "    for i, subset in enumerate(['train', 'valid', 'test']):\n",
    "        m_input = []\n",
    "        sessions = [s for s in subsets.keys() if subset in subsets[s]]\n",
    "        for session in sessions:\n",
    "            selection = selection = (df[\"session\"] == session) & (df[\"variant\"] == variant) & (df[\"subset\"] == subset)\n",
    "            m_input.append(df[selection]['inputwords'].values)\n",
    "        bar_axes = plot_mean_with_std(ax[j], m_input, subset)\n",
    "    ax[j].legend()\n",
    "    ax[j].set_title(f\"Variant: {variant}\")\n",
    "\n",
    "fig.suptitle(f\"Distribution of number of inputwords, per subset and variant\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=3, figsize=(12, 12), sharey=True, sharex=True)\n",
    "\n",
    "def plot_mean_with_std(ax, values, subset, title):\n",
    "    sessions = [s for s in subsets.keys() if subset in subsets[s]]\n",
    "    means = np.array([vals.mean() for vals in values])\n",
    "    stds = np.array([vals.std() for vals in values])\n",
    "    ax.plot(sessions, means, lw=2, label='mean')\n",
    "    ax.fill_between(sessions, means+stds, means-stds, alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "for j, variant in enumerate(variants.keys()):\n",
    "    for i, subset in enumerate(['train', 'valid', 'test']):\n",
    "        m_input = []\n",
    "        sessions = [s for s in subsets.keys() if subset in subsets[s]]\n",
    "        for session in sessions:\n",
    "            selection = selection = (df[\"session\"] == session) & (df[\"variant\"] == variant) & (df[\"subset\"] == subset)\n",
    "            m_input.append(df[selection]['inputwords'].values)\n",
    "        bar_axes = plot_mean_with_std(ax[i][j], m_input, subset, title=f\"Subset: {subset}, {variant}\")\n",
    "\n",
    "fig.suptitle(f\"Distribution of number of inputwords, per subset and variant\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'valid'\n",
    "sessions = [k for k in subsets.keys() if subset in subsets[k]]\n",
    "fig, ax = plt.subplots(ncols=1, nrows=len(sessions), figsize=(12, 12))\n",
    "\n",
    "def plot_hist_bar(ax, values, session, title, bins, range):\n",
    "\n",
    "    for variant, vals in zip(variants.keys(), values): \n",
    "        label = f\"{variant}: \" + r\"$\\mu$\" + f\"={vals.mean():.0f}, \" + r\"$\\sigma$\" + f\"={vals.std():.0f}\"\n",
    "        ax.hist(vals, bins=bins, range=range, alpha=0.5, label=label, density=True)\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "for i, session in enumerate(sessions):\n",
    "    m_input = []\n",
    "    for variant in variants.keys():\n",
    "        selection = selection = (df[\"session\"] == session) & (df[\"variant\"] == variant) & (df[\"subset\"] == subset)\n",
    "        m_input.append(df[selection]['inputwords'].values)\n",
    "    bar_axes = plot_hist_bar(ax[i], m_input, session, title=f\"Session_{session}/{subset}\\nn={len(m_input[0])}\", bins=100, range=(0,2000))\n",
    "\n",
    "fig.suptitle(f\"Distribution of number of inputwords (persona sentences, history, current dialogue)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session=3\n",
    "variant='persona_and_hist'\n",
    "subset='train'\n",
    "\n",
    "selection = (df[\"session\"] == session) & (df[\"variant\"] == variant)& (df[\"subset\"] == subset)\n",
    "ax = df[selection][\"inputwords\"].plot.hist(bins=10, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=len(subsets.keys()), figsize=(12, 12))\n",
    "\n",
    "session=5\n",
    "subset='valid'\n",
    "\n",
    "selection = (df[\"session\"] == session) & (df[\"subset\"] == subset)\n",
    "\n",
    "labels = []\n",
    "for variant in variants.keys():\n",
    "    subset_df = df.loc[selection & (df[\"variant\"] == variant), \"inputwords\"]\n",
    "    labels.append(f\"{variant}: \"+ r\"$\\mu$\" + f\"={subset_df.mean():.0f}, \" + r\"$\\sigma$\" + f\"={subset_df.std():.0f}\")\n",
    "    plt.hist(subset_df, alpha=0.5, label=variant, density=True, bins=40)\n",
    "legend = plt.legend(labels=labels)\n",
    "title = plt.title(f\"Distribution of number of inputwords per variant\\nDataset: session_{session}/{subset}, n={len(subset_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session=4\n",
    "subset='train'\n",
    "\n",
    "selection = (df[\"session\"] == session) & (df[\"subset\"] == subset)\n",
    "\n",
    "df_hist = df[selection]\n",
    "df_hist['inputwords'].hist(by=df_hist['variant'], bins=20, layout=(1,3), figsize=(12,3), density=True, sharey=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using matplottlib - horizontal\n",
    "\n",
    "subset = 'valid'\n",
    "sessions = [k for k in subsets.keys() if subset in subsets[k]]\n",
    "fig, ax = plt.subplots(ncols=1, nrows=len(sessions), figsize=(12, 12))\n",
    "\n",
    "def plot_hist_bar(ax, values, session, title, bins, range):\n",
    "    labels = [f\"{variant}: \" + r\"$\\mu$\" + f\"={vals.mean():.0f}, \" + r\"$\\sigma$\" + f\"={vals.std():.0f}\" for variant, vals in zip(variants.keys(), values)]\n",
    "    n, bins, patches = ax.hist(values, bins=bins, range=range, density=True, label=labels)\n",
    "    legend = ax.legend()\n",
    "    title = ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "for i, session in enumerate(sessions):\n",
    "    m_input = []\n",
    "    for variant in variants.keys():\n",
    "        selection = selection = (df[\"session\"] == session) & (df[\"variant\"] == variant) & (df[\"subset\"] == subset)\n",
    "        m_input.append(df[selection]['inputwords'].values)\n",
    "    bar_axes = plot_hist_bar(ax[i], m_input, session, title=f\"Session_{session}/{subset}\\nn={len(m_input[0])}\", bins=40, range=(0,2000))\n",
    "\n",
    "fig.suptitle(f\"Distribution of number of inputwords (persona sentences, history, current dialogue)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using matplottlib - horizontal\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=len(subsets.keys()), figsize=(12, 12), gridspec_kw={'width_ratios': [10, 4]}, sharey=False)\n",
    "\n",
    "def plot_hist_bar(ax, values, session, title, bins, range):\n",
    "    labels = [f\"{subset}: n={len(vals)}, \" + r\"$\\mu$\" + f\"={vals.mean():.0f}, \" + r\"$\\sigma$\" + f\"={vals.std():.0f}\" for subset, vals in zip(subsets[session], values)]\n",
    "    if session==5:\n",
    "        labels = ['-'] + labels # add empty label\n",
    "        values = [-1] + values # plot a bar, outside the range\n",
    "    n, bins, patches = ax.hist(values, bins=bins, range=range, density=True, label=labels)\n",
    "    legend = ax.legend()\n",
    "    title = ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "for i, session in enumerate(subsets.keys()):\n",
    "    m_input = [np.hstack([[val] * freq for val, freq in m[session]['no_persona_no_hist'][subset]['inputwords_per_sample']]) for subset in subsets[session]]\n",
    "    m_label = [np.hstack([[val] * freq for val, freq in m[session]['no_persona_no_hist'][subset]['labelwords_per_sample'] if val != 0]) for subset in subsets[session]]\n",
    "    bar_axes = plot_hist_bar(ax[i][0], m_input, session, title=f\"Session={session}, input\", bins=25, range=(0,500))\n",
    "    bar_axes = plot_hist_bar(ax[i][1], m_label, session, title=f\"Session={session}, label\", bins=10, range=(0,60))\n",
    "\n",
    "fig.suptitle(\"Distribution of number of words per input sentence (complete dialogue, except last utterance), and label (last utterance)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using matplottlib - horizontal\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=len(subsets.keys()), figsize=(10, 12), sharex=True)\n",
    "\n",
    "def plot_hist_bar(ax, values, session):\n",
    "    labels = [f\"{subset}: n={len(vals)}, \" + r\"$\\mu$\" + f\"={vals.mean():.0f}, \" + r\"$\\sigma$\" + f\"={vals.std():.0f}\" for subset, vals in zip(subsets[session], values)]\n",
    "    if session==5:\n",
    "        labels = ['-'] + labels # add empty label\n",
    "        values = [-1] + values # plot a bar, outside the range\n",
    "    n, bins, patches = ax.hist(values, bins=30, range=(0,500), density=True, label=labels)\n",
    "    legend = ax.legend()\n",
    "    title = ax.set_title(f\"Session={session}\")\n",
    "    return ax\n",
    "\n",
    "for i, session in enumerate(subsets.keys()):\n",
    "    values = [np.hstack([[val] * freq for val, freq in m[session]['no_persona_no_hist'][subset]['inputwords_per_sample']]) for subset in subsets[session]]\n",
    "    bar_axes = plot_hist_bar(ax[i], values, session)\n",
    "\n",
    "fig.suptitle(\"Distribution of number of words per input sentence (complete dialogue, except last utterance)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, nrows=len(subsets.keys()), figsize=(4,12), sharey=True)\n",
    "\n",
    "def plot_hist_bar(ax, values, session):\n",
    "    labels = [f\"{subset}: n={len(vals)}, \" + r\"$\\mu$\" + f\"={vals.mean():.0f}, \" + r\"$\\sigma$\" + f\"={vals.std():.0f}\" for subset, vals in zip(subsets[session], values)]\n",
    "    if session==5:\n",
    "        labels = ['-'] + labels # add empty label\n",
    "        values = [-1] + values # plot a bar, outside the range\n",
    "    n, bins, patches = ax.hist(values, bins=10, range=(0,50), density=True, label=labels)\n",
    "    legend = ax.legend()\n",
    "    title = ax.set_title(f\"Session={session}\")\n",
    "    return ax\n",
    "\n",
    "for i, session in enumerate(subsets.keys()):\n",
    "    values = [\n",
    "        np.hstack([\n",
    "            [val] * freq \n",
    "            for val, freq in m[session]['no_persona_no_hist'][subset]['labelwords_per_sample']\n",
    "            if val != 0 # plot only for sentences that contain a fact\n",
    "        ]) \n",
    "        for subset in subsets[session]\n",
    "    ]\n",
    "    bar_axes = plot_hist_bar(ax[i], values, session)\n",
    "\n",
    "fig.suptitle(\"Distribution of number of words per label sentence (next utterance)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test to draw dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'persona_and_hist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dialogue \u001b[39m=\u001b[39m msc_sessions[\u001b[39m1\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mpersona_and_hist\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m10\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(dialogue)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'persona_and_hist'"
     ]
    }
   ],
   "source": [
    "dialogue = msc_sessions[1]['persona_and_hist']['train'][10][0]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m variant \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mno_persona_no_hist\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Print and plot first dialogue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHistory:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mNext utterance:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39mmsc_sessions[session][variant][subset][\u001b[39m140\u001b[39;49m]))\n\u001b[1;32m      8\u001b[0m msc_sessions[session][variant][subset]\u001b[39m.\u001b[39msave_dialogue_fig(\u001b[39m140\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/PEX/dataset/msc_sessions.py:385\u001b[0m, in \u001b[0;36mMSC_Session.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m    381\u001b[0m \n\u001b[1;32m    382\u001b[0m     \u001b[39m# Compose history and next utterance\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspeaker_prefixes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m         \u001b[39m# Determine who is Speaker 1 and Speaker 2\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m         mapping \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_speaker_mapping(i)\n\u001b[1;32m    386\u001b[0m         \u001b[39m# Put the right prefix before the utterance\u001b[39;00m\n\u001b[1;32m    387\u001b[0m         history \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspeaker_prefixes[mapping[p]] \u001b[39m+\u001b[39m t \u001b[39mfor\u001b[39;00m p, t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[i]]) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/Programming/PEX/dataset/msc_sessions.py:466\u001b[0m, in \u001b[0;36mMSC_Session._get_speaker_mapping\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_speaker_mapping\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Determine who is Speaker 1 and Speaker 2 (who is 'you', who is 'me')\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     speakers \u001b[39m=\u001b[39m [speaker \u001b[39mfor\u001b[39;00m speaker, _ \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory[i] \u001b[39mif\u001b[39;00m speaker \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNobody\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    467\u001b[0m     mapping \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mSpeaker 1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mme\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSpeaker 2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39myou\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNobody\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msessionbreak\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    468\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(speakers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m speakers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSpeaker 1\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "session = 1\n",
    "subset = 'test'\n",
    "variant = 'no_persona_no_hist'\n",
    "\n",
    "# Print and plot first dialogue\n",
    "print(\"History:\\n{}\\nNext utterance:\\n{}\\n\".format(*msc_sessions[session][variant][subset][140]))\n",
    "msc_sessions[session][variant][subset].save_dialogue_fig(140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dialogue\n",
    "session = 1\n",
    "subset = 'test'\n",
    "variant = 'persona_and_hist'\n",
    "dialog_index = 1\n",
    "\n",
    "# Make plot\n",
    "msc_sessions[session][variant][subset].save_dialogue_fig(dialog_index, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msc_sessions[session][variant][subset].indices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "prefixes = configs['default']['speaker_prefixes'] + [configs['default']['sessionbreak_token']]\n",
    "\n",
    "PER_LINE = 0.22 # inch\n",
    "PER_TURN = 0.15 # inch\n",
    "\n",
    "def split_speaker_and_text(turn):\n",
    "    for speaker in prefixes:\n",
    "        prefix_len = len(speaker)\n",
    "        if turn[:prefix_len] == speaker:\n",
    "            return speaker, textwrap.wrap(turn[prefix_len:], width=45)\n",
    "    assert False, f\"None of the speaker prefixes {prefixes} found in turn: {turn}\"\n",
    "\n",
    "\n",
    "def plot_dialogue(turns, next_utterance, title):\n",
    "    wrapped_turns = [split_speaker_and_text(t) for t in turns+ [prefixes[1] + next_utterance]]\n",
    "    total_lines = sum([len(t[1]) for t in wrapped_turns])\n",
    "\n",
    "    # Setup figure\n",
    "    fig_height = 0.5 + len(wrapped_turns) * PER_TURN + total_lines * PER_LINE\n",
    "    fig, ax = plt.subplots(figsize=(6, fig_height))\n",
    "    fig.patch.set_facecolor('ghostwhite')\n",
    "\n",
    "    # Determine triangle coordinates based on figure size\n",
    "    triangle = np.array([[0.02, -0.05/fig_height], [0.05, -0.25/fig_height], [0.12, -0.25/fig_height]])\n",
    "\n",
    "    ypos = 0.2 / fig_height \n",
    "    for i, (speaker, wrapped_turn) in enumerate(wrapped_turns):\n",
    "\n",
    "        # Set alignment alternating left or right\n",
    "        speaker_index = prefixes.index(speaker)\n",
    "        alignment = {0: 'left', 1: 'right', 2: 'center'}[speaker_index]\n",
    "        xpos = {0: 0.05, 1: 0.95, 2: 0.5}[speaker_index]\n",
    "        bbox_style = dict(\n",
    "            boxstyle=\"round\", \n",
    "            fc={0: 'antiquewhite', 1: 'antiquewhite', 2: 'lightsteelblue'}[speaker_index], \n",
    "            ec='tab:blue'\n",
    "        )\n",
    "        if i == len(wrapped_turns) - 1:\n",
    "            bbox_style['fc'] = 'floralwhite'\n",
    "            bbox_style['linestyle'] = '--'\n",
    "\n",
    "        # Plot the text\n",
    "        text = ax.text(xpos, ypos, '\\n'.join(wrapped_turn), \n",
    "            horizontalalignment=alignment,\n",
    "            verticalalignment='top',\n",
    "            wrap=True, \n",
    "            multialignment=alignment,\n",
    "            bbox=bbox_style\n",
    "        )\n",
    "\n",
    "        # Increase ypos, for next utterance, depending on number of lines in current turn\n",
    "        ypos += PER_TURN / fig_height + PER_LINE / fig_height * len(wrapped_turn)\n",
    "\n",
    "        # Add speaker triangle, except for session breaks\n",
    "        if speaker_index != 2:\n",
    "            # Plot triangle below utterance, pointing left or right depending on alignment\n",
    "            if alignment == 'left':\n",
    "                triangle_patch = matplotlib.patches.Polygon(np.array([[0, ypos]]) + triangle)\n",
    "            else:\n",
    "                triangle_patch = matplotlib.patches.Polygon(np.array([[1, ypos]]) + triangle * np.array([[-1, 1]]))\n",
    "            ax.add_patch(triangle_patch)\n",
    "\n",
    "    # Final formatting\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(title)\n",
    "    plt.axis('off')\n",
    "    return fig\n",
    "\n",
    "# Select dialogue\n",
    "session = 4\n",
    "subset = 'valid'\n",
    "variant = 'persona_and_hist'\n",
    "dialog_index = 4\n",
    "dialogue, next_utterance = msc_sessions[session][variant][subset][dialog_index]\n",
    "\n",
    "# Make plot\n",
    "dialog_id = msc_sessions[session][variant][subset].indices[dialog_index]\n",
    "title=f\"Dataset: session_{session}/{subset}, dialog_id: {dialog_id['dialog_id']}\\nvariant: {variant}\"\n",
    "fig = plot_dialogue(dialogue[:-1].split('\\n'), next_utterance, title)  # remove trailing '\\n' from dialogue\n",
    "# print(dialogue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse ngram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MSC_Session.set(speaker_prefixes=['', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for session in subsets.keys():\n",
    "    if session == 1:\n",
    "        version = ['both', 'revised']\n",
    "        session = '-'.join(['1'] + version)\n",
    "    labels[int(str(session)[0])] = {}\n",
    "    option_name = 'no_persona_no_hist'\n",
    "    for subset in subsets[int(str(session)[0])]:\n",
    "        msc = MSC_Session(basedir=basedir, session=session, subset=subset, **variants[option_name])\n",
    "        labels[int(str(session)[0])][subset] = [msc[i][1] for i in range(len(msc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_freq(n, sentence_list):\n",
    "    counter = Counter()\n",
    "    for s in sentence_list:\n",
    "        counter.update(get_ngrams(n, s))\n",
    "    return counter\n",
    "\n",
    "def get_ngrams(n, sentence):\n",
    "    words = sentence.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n):\n",
    "        ngrams.append(tuple(words[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "def get_repeating_ngrams(n, sentence_list):\n",
    "    counter = Counter()\n",
    "    for s in sentence_list:\n",
    "        ngrams = get_ngrams(n, s)\n",
    "        duplicates = [ngram for ngram, freq in Counter(ngrams).items() if freq > 1]\n",
    "        # if len(duplicates) > 0:\n",
    "        #     print(len(s.split()), s)\n",
    "        counter.update(duplicates)\n",
    "    sorted_counter = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n",
    "    return sorted_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = labels[4]['train']\n",
    "# ngrams = get_ngrams_freq(4, all_targets)\n",
    "# ngrams = sorted(ngrams.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "duplicates = get_repeating_ngrams(4, all_targets)\n",
    "len(duplicates)/len(all_targets), len(duplicates), duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in subsets.keys():\n",
    "    for subset in subsets[session]:\n",
    "        duplicates = get_repeating_ngrams(4, labels[session][subset])\n",
    "        print(f\"session_{session:1}/{subset:6} : {len(duplicates):3d}   {len(duplicates)/len(labels[session][subset]):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(16, 3), sharey=True, sharex=True)\n",
    "n_range = np.array([1, 2, 3, 4, 5])\n",
    "for i, session in enumerate(subsets.keys()):\n",
    "    for subset in subsets[session]:\n",
    "        num_duplicates = [len(get_repeating_ngrams(n, labels[session][subset])) for n in n_range]\n",
    "        axs[i].plot(n_range, num_duplicates, label=subset)\n",
    "    axs[i].legend()\n",
    "    axs[i].grid(axis='y', which='major')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(16, 3), sharey=True, sharex=True)\n",
    "n_range = np.array([1, 2, 3, 4, 5])\n",
    "for i, session in enumerate(subsets.keys()):\n",
    "    for subset in subsets[session]:\n",
    "        perc_duplicates = [len(get_repeating_ngrams(n, labels[session][subset])) / len(labels[session][subset]) for n in n_range]\n",
    "        # perc_duplicates = np.array([d/len(labels[session][subset]) for d in num_duplicates])\n",
    "        axs[i].plot(n_range, perc_duplicates, label=subset)\n",
    "    axs[i].legend()\n",
    "    axs[i].grid(axis='y', which='major')\n",
    "    # axs[i].set_ylim(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples as input for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = 5\n",
    "variant = 'persona_and_hist'\n",
    "subset = 'test'\n",
    "dialog_id = 0\n",
    "print(msc_sessions[session][variant][subset][dialog_id][0])\n",
    "print(msc_sessions[session][variant][subset][dialog_id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
