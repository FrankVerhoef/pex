#!/bin/bash

#SBATCH --partition gpu
#SBATCH --nodes 1
#SBATCH --gpus 1
#SBATCH --job-name Tune_GPT
#SBATCH --time 12:00:00
#SBATCH --mem 32G
#SBATCH --output slurm/outputs/train_%A.out

#source ./slurm/.secrets

module purge
module load 2022
module load Anaconda3/2022.05
module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0

source activate pex

#Copy data dir  to scratch
cp -r data  "$TMPDIR"/data
mkdir "$TMPDIR"/logs
mkdir "$TMPDIR"/output

srun python -u run/main.py tune dialogpt dialog \
        --lm gpt2 \
	--basedir msc/msc_dialogue/  \
        --session 4  \
	--speaker_prefixes \<other\> \<self\> \
	--include_history \
	--include_persona \
	--input_order history-personas-current \
	--train_samples 2000 \
	--valid_samples 100 \
	--augmented \
        --batch_size 8  \
        --learning_rate 0.00005 \
        --epochs 1  \
	--valid_interval 25 \
	--patience 4 \
        --device cuda  \
        --loglevel INFO  \
        --logdir "$TMPDIR"/logs/  \
	--datadir "$TMPDIR"/data/ \
        --output_dir "$TMPDIR"/output/ \
	--checkpoint_dir ~/pex/checkpoints/ \
	--experiment_name gpt_context4 \

cp "$TMPDIR"/logs/* ./logs
cp -r "$TMPDIR"/output/* ./output
