#!/bin/bash

#SBATCH --partition gpu
#SBATCH --nodes 1
#SBATCH --gpus 1
#SBATCH --job-name Train_T5
#SBATCH --time 02:30:00
#SBATCH --mem 32G
#SBATCH --output slurm/outputs/train_%A.out

source ./slurm/.secrets

module purge
module load 2022
module load Anaconda3/2022.05
module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0
module load Java/11.0.16

source activate pex

#Copy data dir  to scratch
cp -r data  "$TMPDIR"/data
mkdir "$TMPDIR"/logs
mkdir "$TMPDIR"/checkpoints
mkdir "$TMPDIR"/output

srun python -u run/main.py train t5 generate \
	--t5_base t5-base \
	--basedir msc/msc_personasummary/  \
	--sessions 1 2 3 \
	--speaker_prefixes \[other\] \[self\] \
	--train_samples 4000 \
	--valid_samples 200 \
	--skip_eval \
	--batch_size 48  \
	--learning_rate 0.0001 \
	--lm_loss_factor 0.5 \
	--epochs 5  \
	--valid_interval 25 \
	--patience 9 \
	--device cuda  \
	--loglevel VERBOSE  \
	--logdir "$TMPDIR"/logs/  \
	--datadir "$TMPDIR"/data/ \
    --checkpoint_dir "$TMPDIR"/checkpoints/ \
	--output_dir "$TMPDIR"/output/ \
    --save trained_nll05final  \
	--seed 2206 \
    --use_wandb

cp "$TMPDIR"/logs/* ./logs
cp "$TMPDIR"/checkpoints/* ./checkpoints
cp "$TMPDIR"/output/* ./output

